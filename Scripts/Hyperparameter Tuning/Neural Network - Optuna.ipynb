{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2b4cbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import median_absolute_deviation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import tensorflow_addons as tfa\n",
    "import random\n",
    "from keras.regularizers import l2\n",
    "random_state = 1234\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "import optuna\n",
    "from pathlib import Path\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6f70e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48f53805",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = Path(os.getcwd()).parent.parent\n",
    "cross_validation_dir = os.path.join(parent_dir, \"Data\", \"train_test_indices.npy\")\n",
    "# dataset = \"MSK_Impact_train\"\n",
    "dataset = \"BCAST_train\"\n",
    "data_dir = os.path.join(parent_dir, \"Data\" , dataset)\n",
    "train_test_indices = np.load(cross_validation_dir, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93e35b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_dir(dirname):\n",
    "    dir_files = list()\n",
    "    for root, _, files in os.walk(dirname):\n",
    "        for file in files:\n",
    "            dir_files.append(os.path.join(root, file))\n",
    "    \n",
    "    return dir_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4868ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(input_shape=(96,), n_hidden_layers=2, n_hidden_nodes=16, \n",
    "                    activation=\"relu\", learning_rate=0.001, weight_decay = 0, l2_kernel = 0.01, l2_bias = 0.01):\n",
    "    # optimizer parameters\n",
    "    loss = \"binary_crossentropy\"\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay = weight_decay)\n",
    "    metrics = keras.metrics.AUC(name='auc')\n",
    "    \n",
    "    # ANN model\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "    for _ in range(n_hidden_layers):\n",
    "        model.add(keras.layers.Dense(n_hidden_nodes, activation=activation, kernel_regularizer=l2(l2_kernel), bias_regularizer=l2(l2_bias)))\n",
    "    \n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    # optimizer\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4d22a3f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_row_indices_with_sum_zero(X):\n",
    "    return X.index[(X.sum(axis=1) == 0)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b835051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_rows(X):\n",
    "    return X.div(X.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a80587c5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dataset_generator(data_dir, num_files, y_col):\n",
    "\n",
    "  for datafile in random.sample(list_files_in_dir(data_dir), num_files):\n",
    "      data = pd.read_csv(datafile)\n",
    "      X = data.iloc[:, :96]\n",
    "      y = (data[y_col]).astype(np.int_)\n",
    "      X.columns = X.columns.str.replace('[', '').str.replace(']', '').str.replace('>', '')\n",
    "#           print(X.sum(axis=0))\n",
    "      X = scale_rows(X)\n",
    "#           print(X.sum(axis=1))\n",
    "      yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b05e467",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def scale_data(df):\n",
    "    # Define the scaler \n",
    "    scaler = StandardScaler().fit(df)\n",
    "    # Scales each individual row   \n",
    "    df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "893fb89e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_val_test_generator(data_dir, num_samples, y_col, test_frac=0.1, n_folds = 10):\n",
    "    val_frac = test_frac/(1.0 - test_frac)\n",
    "    \n",
    "    for X, y in dataset_generator(data_dir, num_samples, y_col):\n",
    "        fold_data = list()\n",
    "        skf = StratifiedKFold(n_splits=n_folds, shuffle = True)\n",
    "#         skf = StratifiedKFold(n_splits=num_folds)\n",
    "        \n",
    "        for train_val_index, test_index in skf.split(X, y):\n",
    "            X_test, y_test = X.iloc[test_index, :], y.iloc[test_index]\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X.iloc[train_val_index,:], y.iloc[train_val_index], \n",
    "                test_size=val_frac, \n",
    "                random_state=random_state, \n",
    "                stratify=y.iloc[train_val_index]\n",
    "            )\n",
    "            \n",
    "            #X_train, X_val, X_test = scale_columns(X_train, X_val, X_test)\n",
    "            fold_data.append(((X_train, y_train), \n",
    "                              (X_val, y_val), \n",
    "                              (X_test, y_test)))\n",
    "            \n",
    "        yield fold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74a44f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_generator_default(data_dir, num_samples, y_col, test_frac=0.1):\n",
    "    \n",
    "    val_frac = test_frac/(1.0 - test_frac)\n",
    "    \n",
    "    for X, y in dataset_generator(data_dir, num_samples, y_col):\n",
    "        fold_data = list()\n",
    "        for train_val_index, test_index in train_test_indices:\n",
    "            X_test, y_test = X.iloc[test_index, :], y.iloc[test_index]\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X.iloc[train_val_index,:], y.iloc[train_val_index], \n",
    "                test_size=val_frac, \n",
    "                random_state=random_state, \n",
    "                stratify=y.iloc[train_val_index]\n",
    "            )\n",
    "#             print(train_val_index, test_index)\n",
    "            \n",
    "            #X_train, X_val, X_test = scale_columns(X_train, X_val, X_test)\n",
    "            fold_data.append(((X_train, y_train), \n",
    "                              (X_val, y_val), \n",
    "                              (X_test, y_test)))\n",
    "            \n",
    "        yield fold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7300da92",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compute_mlp_performance(trial, input_shape=(96,), data_dir=data_dir, n_folds=10, class_weight_0 = 1, class_weight_1 = 1, num_samples = 1, y_col = 'is_sig3_20'):\n",
    "    aucs = list()\n",
    "    models = list()\n",
    "    \n",
    "    # 60-20-20 split\n",
    "    test_frac=1.0/float(n_folds)\n",
    "#     fpr_list = []\n",
    "#     tpr_list = []\n",
    "#     roc_auc_list = []\n",
    "    \n",
    "    for folds_data in train_val_test_generator(data_dir, num_samples=num_samples, y_col=y_col):\n",
    "        fold_aucs = list()\n",
    "        fold_models = list()\n",
    "        \n",
    "        for fold_data in folds_data:\n",
    "            # get data\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test) = fold_data\n",
    "            # build model and ensure that parameters passed in are within the normal range\n",
    "            # if we don't type cast as integers, bayesian optimizer will guess float values\n",
    "            model = build_mlp_model(input_shape, \n",
    "                                    trial.suggest_int('n_hidden_layers', 1,3), \n",
    "                                    trial.suggest_int('n_hidden_nodes', 20, 300), \n",
    "                                    trial.suggest_categorical(\"activation\", [\"relu\", \"sigmoid\", \"softmax\"]), \n",
    "                                    trial.suggest_float('learning_rate', 1e-9, 1e-1),\n",
    "                                    weight_decay = trial.suggest_float('weight_decay', 0, 5e-1),\n",
    "                                    l2_kernel = trial.suggest_float('l2_kernel', 0, 5e-1),\n",
    "                                    l2_bias = trial.suggest_float('l2_bias', 0, 5e-1))\n",
    "            model.fit(X_train, y_train, \n",
    "                      validation_data=(X_val, y_val), \n",
    "                      epochs=1000, batch_size=32, verbose=0,\n",
    "                      class_weight= {0 : trial.suggest_float('class_weight_0', 0, 5), 1 : trial.suggest_float('class_weight_1', 0, 5)},\n",
    "                      callbacks=[keras.callbacks.EarlyStopping(monitor='val_auc', patience=5)])\n",
    "            \n",
    "            # evaluate\n",
    "            y_score = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "            fold_aucs.append(y_score)\n",
    "            fold_models.append(model)\n",
    "        aucs.append(fold_aucs)\n",
    "        models.append(fold_models)\n",
    "        \n",
    "    # Gets median index value for all the different samples (rows)  \n",
    "    medianIndices = [indices[len(aucs[0])//2] for indices in np.argsort(aucs, axis=1)]\n",
    "    medianValues = [values[index] for values, index in zip(aucs, medianIndices)]\n",
    "    \n",
    "    # Gets the file which contains the median of median value\n",
    "    fileInd = np.argsort(medianValues)[len(medianValues)//2]\n",
    "    \n",
    "    aucs = np.array(aucs)\n",
    "    \n",
    "    median_of_median_model = models[fileInd][medianIndices[fileInd]]\n",
    "    median_of_median_auc = np.median(np.median(aucs, axis=1))\n",
    "    mad_of_mad_auc = median_absolute_deviation(aucs, axis=1)\n",
    "#     return median_of_median_auc, mad_of_mad_auc\n",
    "    return median_of_median_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c64b831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BCAST_CV_indices = []\n",
    "for X, y in dataset_generator(data_dir, 1, 'is_sig3'):\n",
    "    fold_data = list()\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle = True)\n",
    "#         skf = StratifiedKFold(n_splits=num_folds)\n",
    "\n",
    "    for train_val_index, test_index in skf.split(X, y):\n",
    "        BCAST_CV_indices.append([train_val_index, test_index])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b61cf7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  14,\n",
       "          15,  17,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,\n",
       "          30,  31,  32,  33,  35,  36,  38,  40,  41,  42,  43,  44,  45,\n",
       "          46,  47,  49,  50,  51,  53,  54,  55,  57,  58,  61,  62,  64,\n",
       "          65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "          78,  80,  81,  82,  83,  84,  86,  87,  89,  90,  91,  92,  93,\n",
       "          94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,\n",
       "         107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n",
       "         121, 123, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136,\n",
       "         138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n",
       "         151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 163, 164, 165,\n",
       "         166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
       "         179, 180, 181, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193,\n",
       "         194, 195, 196, 197, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "         209, 210, 211, 212, 214, 215, 217, 218, 220, 221, 223, 225, 226,\n",
       "         227, 228, 229, 230, 231, 232, 234, 235, 236, 237, 238, 239, 240,\n",
       "         241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
       "         254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267,\n",
       "         268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
       "         281, 282, 283, 284, 285, 286, 288, 289, 290, 291, 292, 293, 294,\n",
       "         295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
       "         308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321,\n",
       "         322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334,\n",
       "         335, 336, 338, 339, 340, 342, 343, 344, 345, 346, 347, 348, 349,\n",
       "         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 363, 365,\n",
       "         366, 367, 368, 369, 370, 372, 373, 374, 376, 378, 379, 380, 381,\n",
       "         382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394,\n",
       "         396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408,\n",
       "         410, 411, 412, 413, 414, 416, 418, 419, 420, 422, 423, 424, 425,\n",
       "         426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438,\n",
       "         439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
       "         452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "         466, 467, 468, 469, 470, 471, 472, 473, 475, 476, 477, 478, 479,\n",
       "         480, 481, 482, 483, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "         494, 495, 496, 497, 499, 500, 501, 502, 503, 504, 505, 506, 507,\n",
       "         508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 519, 520, 522,\n",
       "         523, 524, 525, 526, 531, 532, 533, 534, 535, 536, 537, 538, 539,\n",
       "         540, 541, 542, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553,\n",
       "         554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566,\n",
       "         567, 568, 569, 570, 571, 572, 574, 575, 576, 577, 578, 579, 580,\n",
       "         581, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594,\n",
       "         595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607,\n",
       "         608, 609, 612, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "         624, 625, 626, 627, 629, 630, 631, 632, 633, 634, 635, 636, 637,\n",
       "         638, 639, 640, 641, 642, 643, 645, 646, 647, 648, 649, 651, 652,\n",
       "         653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665,\n",
       "         666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677]),\n",
       "  array([ 12,  13,  16,  18,  34,  37,  39,  48,  52,  56,  59,  60,  63,\n",
       "          79,  85,  88, 111, 122, 124, 132, 137, 158, 162, 182, 186, 198,\n",
       "         199, 213, 216, 219, 222, 224, 233, 264, 287, 318, 337, 341, 360,\n",
       "         362, 364, 371, 375, 377, 395, 409, 415, 417, 421, 455, 474, 484,\n",
       "         498, 518, 521, 527, 528, 529, 530, 543, 573, 582, 610, 611, 613,\n",
       "         628, 644, 650])],\n",
       " [array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "          13,  14,  15,  16,  17,  18,  19,  20,  22,  23,  24,  25,  26,\n",
       "          27,  28,  29,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
       "          41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
       "          54,  55,  56,  57,  58,  59,  60,  61,  63,  64,  65,  66,  67,\n",
       "          68,  70,  71,  72,  73,  74,  75,  77,  78,  79,  80,  81,  82,\n",
       "          83,  84,  85,  86,  87,  88,  89,  90,  91,  93,  94,  95,  96,\n",
       "          98,  99, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112,\n",
       "         113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "         126, 127, 128, 130, 131, 132, 135, 136, 137, 138, 139, 140, 141,\n",
       "         142, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158,\n",
       "         159, 160, 161, 162, 164, 166, 168, 169, 170, 171, 172, 173, 174,\n",
       "         175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187,\n",
       "         188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201,\n",
       "         202, 203, 204, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216,\n",
       "         217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229,\n",
       "         230, 231, 233, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244,\n",
       "         246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259,\n",
       "         260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "         274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287,\n",
       "         288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301,\n",
       "         303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316,\n",
       "         317, 318, 319, 320, 321, 323, 324, 327, 329, 330, 331, 332, 333,\n",
       "         334, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348,\n",
       "         351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "         364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "         377, 378, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
       "         391, 392, 393, 394, 395, 396, 397, 399, 400, 401, 403, 404, 405,\n",
       "         406, 407, 408, 409, 410, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "         420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
       "         433, 434, 435, 436, 437, 440, 441, 442, 443, 444, 446, 447, 448,\n",
       "         449, 450, 451, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,\n",
       "         463, 464, 465, 466, 468, 469, 470, 471, 473, 474, 475, 476, 477,\n",
       "         478, 479, 480, 481, 483, 484, 485, 487, 488, 489, 490, 491, 492,\n",
       "         494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "         507, 508, 509, 510, 511, 512, 513, 515, 516, 517, 518, 519, 520,\n",
       "         521, 522, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534,\n",
       "         535, 536, 537, 538, 539, 541, 542, 543, 544, 545, 546, 548, 549,\n",
       "         550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,\n",
       "         563, 564, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576,\n",
       "         577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 588, 589, 590,\n",
       "         592, 593, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605,\n",
       "         606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 619, 620,\n",
       "         621, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634,\n",
       "         635, 636, 637, 638, 639, 640, 642, 643, 644, 645, 646, 648, 649,\n",
       "         650, 651, 652, 653, 654, 655, 657, 658, 659, 661, 662, 663, 664,\n",
       "         665, 666, 667, 668, 670, 671, 672, 673, 674, 675, 676, 677]),\n",
       "  array([ 21,  30,  62,  69,  76,  92,  97, 100, 105, 129, 133, 134, 143,\n",
       "         144, 145, 157, 163, 165, 167, 197, 205, 211, 232, 234, 245, 252,\n",
       "         262, 281, 299, 302, 311, 322, 325, 326, 328, 335, 339, 349, 350,\n",
       "         379, 398, 402, 411, 438, 439, 445, 452, 467, 472, 482, 486, 493,\n",
       "         514, 523, 540, 547, 565, 587, 591, 594, 617, 618, 622, 641, 647,\n",
       "         656, 660, 669])],\n",
       " [array([  0,   1,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          15,  16,  17,  18,  19,  20,  21,  23,  24,  25,  26,  27,  28,\n",
       "          29,  30,  31,  33,  34,  36,  37,  38,  39,  40,  41,  43,  44,\n",
       "          45,  46,  47,  48,  50,  51,  52,  53,  54,  55,  56,  57,  58,\n",
       "          59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  72,\n",
       "          74,  75,  76,  77,  79,  80,  81,  82,  83,  84,  85,  86,  87,\n",
       "          88,  89,  90,  91,  92,  94,  95,  96,  97,  99, 100, 101, 102,\n",
       "         103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
       "         116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "         130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143,\n",
       "         144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157,\n",
       "         158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171,\n",
       "         172, 173, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186,\n",
       "         187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199,\n",
       "         200, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 215,\n",
       "         216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230,\n",
       "         231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244,\n",
       "         245, 246, 248, 249, 250, 252, 253, 254, 256, 257, 258, 259, 260,\n",
       "         261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274,\n",
       "         275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290,\n",
       "         291, 292, 293, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305,\n",
       "         306, 307, 308, 309, 311, 312, 314, 315, 316, 317, 318, 319, 320,\n",
       "         321, 322, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335,\n",
       "         336, 337, 338, 339, 340, 341, 343, 344, 345, 346, 347, 348, 349,\n",
       "         350, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363,\n",
       "         364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "         377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "         390, 391, 392, 393, 394, 395, 396, 398, 399, 400, 402, 403, 404,\n",
       "         405, 406, 407, 408, 409, 410, 411, 413, 414, 415, 416, 417, 418,\n",
       "         419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
       "         432, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445,\n",
       "         446, 447, 448, 449, 450, 452, 453, 454, 455, 457, 460, 461, 462,\n",
       "         464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477,\n",
       "         478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490,\n",
       "         491, 492, 493, 494, 495, 497, 498, 499, 500, 501, 502, 503, 504,\n",
       "         505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 517, 518,\n",
       "         520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "         533, 534, 535, 536, 537, 538, 539, 540, 542, 543, 544, 545, 546,\n",
       "         547, 548, 549, 550, 551, 552, 553, 554, 555, 557, 558, 559, 560,\n",
       "         562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574,\n",
       "         576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588,\n",
       "         590, 591, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604,\n",
       "         605, 606, 607, 608, 609, 610, 611, 612, 613, 615, 616, 617, 618,\n",
       "         619, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 633, 634,\n",
       "         635, 636, 637, 638, 639, 641, 643, 644, 645, 646, 647, 649, 650,\n",
       "         651, 652, 653, 655, 656, 657, 658, 659, 660, 661, 663, 664, 665,\n",
       "         666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677]),\n",
       "  array([  2,  14,  22,  32,  35,  42,  49,  71,  73,  78,  93,  98, 117,\n",
       "         136, 150, 169, 174, 179, 201, 206, 214, 218, 228, 239, 247, 251,\n",
       "         255, 266, 277, 278, 289, 294, 304, 310, 313, 323, 332, 342, 359,\n",
       "         397, 401, 412, 433, 451, 456, 458, 459, 463, 466, 496, 516, 519,\n",
       "         541, 556, 561, 575, 589, 592, 593, 614, 620, 621, 632, 640, 642,\n",
       "         648, 654, 662])],\n",
       " [array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,\n",
       "          14,  16,  17,  18,  19,  20,  21,  22,  23,  25,  26,  27,  28,\n",
       "          29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "          57,  58,  59,  60,  62,  63,  64,  65,  66,  67,  68,  69,  71,\n",
       "          72,  73,  74,  76,  77,  78,  79,  80,  81,  82,  83,  85,  86,\n",
       "          87,  88,  89,  90,  91,  92,  93,  94,  96,  97,  98,  99, 100,\n",
       "         101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114,\n",
       "         115, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129,\n",
       "         130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "         144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157,\n",
       "         158, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171,\n",
       "         172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185,\n",
       "         186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
       "         199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
       "         213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 227,\n",
       "         228, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243,\n",
       "         244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256,\n",
       "         257, 258, 259, 260, 261, 262, 263, 264, 266, 267, 268, 269, 271,\n",
       "         272, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "         286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 298, 299, 300,\n",
       "         302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314,\n",
       "         315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327,\n",
       "         328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 341, 342,\n",
       "         345, 346, 347, 348, 349, 350, 353, 354, 355, 358, 359, 360, 361,\n",
       "         362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374,\n",
       "         375, 376, 377, 378, 379, 380, 381, 383, 384, 385, 386, 387, 388,\n",
       "         390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "         403, 404, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417,\n",
       "         418, 419, 420, 421, 422, 423, 424, 425, 428, 431, 433, 434, 435,\n",
       "         436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 447, 448, 449,\n",
       "         450, 451, 452, 453, 454, 455, 456, 458, 459, 460, 461, 462, 463,\n",
       "         464, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477,\n",
       "         478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 490, 491,\n",
       "         492, 493, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505,\n",
       "         506, 507, 508, 509, 510, 512, 513, 514, 515, 516, 518, 519, 521,\n",
       "         522, 523, 524, 525, 527, 528, 529, 530, 531, 532, 533, 534, 535,\n",
       "         536, 537, 538, 539, 540, 541, 543, 544, 545, 546, 547, 548, 550,\n",
       "         551, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
       "         565, 566, 567, 568, 569, 570, 573, 574, 575, 577, 578, 580, 581,\n",
       "         582, 583, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595,\n",
       "         596, 598, 600, 601, 602, 603, 604, 605, 606, 608, 609, 610, 611,\n",
       "         613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 625, 626,\n",
       "         627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639,\n",
       "         640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652,\n",
       "         653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665,\n",
       "         666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677]),\n",
       "  array([ 10,  15,  24,  46,  47,  61,  70,  75,  84,  95, 110, 116, 123,\n",
       "         135, 155, 159, 177, 203, 225, 226, 229, 230, 238, 265, 270, 273,\n",
       "         292, 297, 301, 338, 340, 343, 344, 351, 352, 356, 357, 382, 389,\n",
       "         405, 406, 426, 427, 429, 430, 432, 446, 457, 465, 489, 494, 511,\n",
       "         517, 520, 526, 542, 549, 552, 571, 572, 576, 579, 584, 597, 599,\n",
       "         607, 612, 624])],\n",
       " [array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  26,  27,\n",
       "          28,  29,  30,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  97,\n",
       "          98, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112,\n",
       "         113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
       "         127, 129, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143,\n",
       "         144, 145, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158,\n",
       "         159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172,\n",
       "         173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186,\n",
       "         187, 188, 189, 192, 193, 194, 195, 197, 198, 199, 201, 202, 203,\n",
       "         205, 206, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
       "         220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232,\n",
       "         233, 234, 235, 236, 237, 238, 239, 240, 241, 244, 245, 247, 248,\n",
       "         249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262,\n",
       "         263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275,\n",
       "         276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 287, 288, 289,\n",
       "         290, 292, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304,\n",
       "         305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317,\n",
       "         318, 319, 320, 321, 322, 323, 325, 326, 327, 328, 329, 330, 332,\n",
       "         334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346,\n",
       "         347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359,\n",
       "         360, 361, 362, 363, 364, 365, 366, 367, 369, 370, 371, 372, 373,\n",
       "         374, 375, 377, 378, 379, 380, 382, 383, 384, 385, 387, 388, 389,\n",
       "         390, 391, 392, 393, 394, 395, 396, 397, 398, 401, 402, 403, 404,\n",
       "         405, 406, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "         421, 422, 423, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n",
       "         436, 438, 439, 440, 442, 443, 444, 445, 446, 447, 448, 449, 451,\n",
       "         452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 463, 464, 465,\n",
       "         466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479,\n",
       "         480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 492, 493, 494,\n",
       "         495, 496, 498, 499, 500, 501, 503, 504, 505, 506, 507, 509, 510,\n",
       "         511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523,\n",
       "         524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536,\n",
       "         537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549,\n",
       "         550, 551, 552, 553, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n",
       "         564, 565, 566, 567, 568, 570, 571, 572, 573, 574, 575, 576, 577,\n",
       "         578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 591,\n",
       "         592, 593, 594, 596, 597, 598, 599, 600, 601, 603, 604, 605, 606,\n",
       "         607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619,\n",
       "         620, 621, 622, 623, 624, 625, 626, 628, 629, 631, 632, 633, 634,\n",
       "         635, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
       "         650, 651, 652, 653, 654, 655, 656, 657, 659, 660, 662, 664, 665,\n",
       "         666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677]),\n",
       "  array([  0,  25,  31,  55,  66,  83,  96,  99, 107, 114, 128, 130, 131,\n",
       "         141, 146, 153, 161, 185, 190, 191, 196, 200, 204, 207, 209, 242,\n",
       "         243, 246, 257, 283, 291, 298, 324, 331, 333, 368, 376, 381, 386,\n",
       "         399, 400, 407, 408, 420, 424, 425, 437, 441, 450, 462, 475, 490,\n",
       "         491, 497, 502, 508, 554, 569, 590, 595, 602, 627, 630, 636, 637,\n",
       "         658, 661, 663])],\n",
       " [array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "          13,  14,  15,  16,  17,  18,  20,  21,  22,  23,  24,  25,  26,\n",
       "          27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "          41,  42,  43,  45,  46,  47,  48,  49,  50,  52,  53,  54,  55,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
       "          69,  70,  71,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
       "          83,  84,  85,  86,  87,  88,  89,  90,  92,  93,  95,  96,  97,\n",
       "          98,  99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139,\n",
       "         140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 153, 155, 156,\n",
       "         157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "         171, 172, 173, 174, 175, 176, 177, 178, 179, 182, 184, 185, 186,\n",
       "         187, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200,\n",
       "         201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214,\n",
       "         215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229,\n",
       "         230, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244,\n",
       "         245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258,\n",
       "         259, 260, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "         273, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 287,\n",
       "         288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300,\n",
       "         301, 302, 304, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
       "         317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330,\n",
       "         331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343,\n",
       "         344, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358,\n",
       "         359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 371, 372,\n",
       "         373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 385, 386, 387,\n",
       "         388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400,\n",
       "         401, 402, 403, 404, 405, 406, 407, 408, 409, 411, 412, 413, 415,\n",
       "         416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "         429, 430, 432, 433, 434, 435, 437, 438, 439, 440, 441, 442, 443,\n",
       "         444, 445, 446, 448, 450, 451, 452, 453, 454, 455, 456, 457, 458,\n",
       "         459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471,\n",
       "         472, 473, 474, 475, 476, 477, 479, 480, 482, 484, 485, 486, 487,\n",
       "         488, 489, 490, 491, 492, 493, 494, 496, 497, 498, 499, 500, 501,\n",
       "         502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514,\n",
       "         515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527,\n",
       "         528, 529, 530, 532, 533, 534, 535, 536, 537, 538, 540, 541, 542,\n",
       "         543, 544, 545, 546, 547, 548, 549, 551, 552, 554, 555, 556, 557,\n",
       "         558, 559, 560, 561, 563, 564, 565, 567, 568, 569, 570, 571, 572,\n",
       "         573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 587,\n",
       "         588, 589, 590, 591, 592, 593, 594, 595, 597, 598, 599, 601, 602,\n",
       "         603, 604, 605, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,\n",
       "         617, 618, 620, 621, 622, 623, 624, 626, 627, 628, 630, 632, 634,\n",
       "         636, 637, 638, 639, 640, 641, 642, 643, 644, 646, 647, 648, 649,\n",
       "         650, 653, 654, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665,\n",
       "         666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677]),\n",
       "  array([ 19,  40,  44,  51,  72,  91,  94, 102, 112, 138, 147, 148, 152,\n",
       "         154, 170, 180, 181, 183, 194, 210, 217, 220, 231, 240, 254, 261,\n",
       "         274, 286, 303, 305, 306, 327, 345, 346, 370, 383, 384, 410, 414,\n",
       "         431, 436, 447, 449, 478, 481, 483, 495, 531, 539, 550, 553, 562,\n",
       "         566, 585, 586, 596, 600, 606, 619, 625, 629, 631, 633, 635, 645,\n",
       "         651, 652, 655])],\n",
       " [array([  0,   1,   2,   3,   4,   7,   8,  10,  11,  12,  13,  14,  15,\n",
       "          16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "          29,  30,  31,  32,  33,  34,  35,  36,  37,  39,  40,  42,  43,\n",
       "          44,  45,  46,  47,  48,  49,  51,  52,  53,  54,  55,  56,  57,\n",
       "          59,  60,  61,  62,  63,  64,  65,  66,  68,  69,  70,  71,  72,\n",
       "          73,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  88,\n",
       "          91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 102, 103, 104,\n",
       "         105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118,\n",
       "         119, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133,\n",
       "         134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146,\n",
       "         147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160,\n",
       "         161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 174, 175,\n",
       "         176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188,\n",
       "         189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 203,\n",
       "         204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 216, 217,\n",
       "         218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231,\n",
       "         232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "         247, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262,\n",
       "         263, 264, 265, 266, 267, 268, 269, 270, 272, 273, 274, 275, 276,\n",
       "         277, 278, 279, 280, 281, 282, 283, 285, 286, 287, 288, 289, 290,\n",
       "         291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303,\n",
       "         304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 318,\n",
       "         320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n",
       "         334, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 348,\n",
       "         349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361,\n",
       "         362, 364, 365, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "         378, 379, 380, 381, 382, 383, 384, 386, 388, 389, 390, 391, 392,\n",
       "         393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
       "         406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 420,\n",
       "         421, 423, 424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 436,\n",
       "         437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 449, 450,\n",
       "         451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 465,\n",
       "         466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
       "         480, 481, 482, 483, 484, 485, 486, 488, 489, 490, 491, 492, 493,\n",
       "         494, 495, 496, 497, 498, 499, 501, 502, 503, 504, 505, 506, 508,\n",
       "         509, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522,\n",
       "         523, 524, 525, 526, 527, 528, 529, 530, 531, 533, 534, 535, 537,\n",
       "         538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
       "         552, 553, 554, 555, 556, 557, 561, 562, 563, 564, 565, 566, 567,\n",
       "         568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 579, 580, 581,\n",
       "         582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594,\n",
       "         595, 596, 597, 599, 600, 602, 603, 604, 605, 606, 607, 608, 609,\n",
       "         610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622,\n",
       "         623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635,\n",
       "         636, 637, 639, 640, 641, 642, 644, 645, 647, 648, 649, 650, 651,\n",
       "         652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664,\n",
       "         665, 666, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677]),\n",
       "  array([  5,   6,   9,  38,  41,  50,  58,  67,  74,  86,  87,  89,  90,\n",
       "         101, 115, 120, 127, 156, 168, 173, 193, 202, 208, 223, 235, 237,\n",
       "         248, 249, 250, 271, 284, 316, 317, 319, 321, 336, 347, 363, 366,\n",
       "         367, 385, 387, 418, 419, 422, 428, 435, 448, 454, 464, 479, 487,\n",
       "         500, 507, 510, 532, 536, 548, 558, 559, 560, 578, 598, 601, 638,\n",
       "         643, 646, 667])],\n",
       " [array([  0,   1,   2,   3,   4,   5,   6,   9,  10,  11,  12,  13,  14,\n",
       "          15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "          43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  55,  56,\n",
       "          58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "          71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,\n",
       "          97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
       "         110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122,\n",
       "         123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136,\n",
       "         137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n",
       "         151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163,\n",
       "         164, 165, 166, 167, 168, 169, 170, 173, 174, 176, 177, 179, 180,\n",
       "         181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193,\n",
       "         194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206,\n",
       "         207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
       "         220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232,\n",
       "         233, 234, 235, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247,\n",
       "         248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261,\n",
       "         262, 264, 265, 266, 268, 270, 271, 272, 273, 274, 276, 277, 278,\n",
       "         279, 280, 281, 283, 284, 286, 287, 288, 289, 290, 291, 292, 294,\n",
       "         295, 296, 297, 298, 299, 301, 302, 303, 304, 305, 306, 309, 310,\n",
       "         311, 312, 313, 314, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "         325, 326, 327, 328, 329, 331, 332, 333, 335, 336, 337, 338, 339,\n",
       "         340, 341, 342, 343, 344, 345, 346, 347, 349, 350, 351, 352, 355,\n",
       "         356, 357, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 370,\n",
       "         371, 372, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n",
       "         386, 387, 388, 389, 391, 392, 395, 396, 397, 398, 399, 400, 401,\n",
       "         402, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 417,\n",
       "         418, 419, 420, 421, 422, 424, 425, 426, 427, 428, 429, 430, 431,\n",
       "         432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444,\n",
       "         445, 446, 447, 448, 449, 450, 451, 452, 454, 455, 456, 457, 458,\n",
       "         459, 462, 463, 464, 465, 466, 467, 469, 470, 471, 472, 474, 475,\n",
       "         477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 489, 490, 491,\n",
       "         492, 493, 494, 495, 496, 497, 498, 499, 500, 502, 504, 507, 508,\n",
       "         510, 511, 512, 513, 514, 516, 517, 518, 519, 520, 521, 522, 523,\n",
       "         524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 536, 537,\n",
       "         538, 539, 540, 541, 542, 543, 544, 546, 547, 548, 549, 550, 551,\n",
       "         552, 553, 554, 556, 557, 558, 559, 560, 561, 562, 564, 565, 566,\n",
       "         568, 569, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581,\n",
       "         582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594,\n",
       "         595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607,\n",
       "         608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620,\n",
       "         621, 622, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634,\n",
       "         635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647,\n",
       "         648, 649, 650, 651, 652, 654, 655, 656, 657, 658, 659, 660, 661,\n",
       "         662, 663, 664, 665, 666, 667, 669, 671, 673, 675, 676, 677]),\n",
       "  array([  7,   8,  28,  29,  54,  57, 125, 140, 171, 172, 175, 178, 236,\n",
       "         244, 258, 263, 267, 269, 275, 282, 285, 293, 300, 307, 308, 315,\n",
       "         330, 334, 348, 353, 354, 358, 369, 373, 374, 390, 393, 394, 403,\n",
       "         404, 416, 423, 453, 460, 461, 468, 473, 476, 480, 488, 501, 503,\n",
       "         505, 506, 509, 515, 535, 545, 555, 563, 567, 570, 623, 653, 668,\n",
       "         670, 672, 674])],\n",
       " [array([  0,   2,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,\n",
       "          16,  18,  19,  20,  21,  22,  24,  25,  26,  28,  29,  30,  31,\n",
       "          32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,\n",
       "          45,  46,  47,  48,  49,  50,  51,  52,  54,  55,  56,  57,  58,\n",
       "          59,  60,  61,  62,  63,  64,  66,  67,  68,  69,  70,  71,  72,\n",
       "          73,  74,  75,  76,  77,  78,  79,  81,  82,  83,  84,  85,  86,\n",
       "          87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,\n",
       "         100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 114, 115, 116,\n",
       "         117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "         130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143,\n",
       "         144, 145, 146, 147, 148, 150, 152, 153, 154, 155, 156, 157, 158,\n",
       "         159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172,\n",
       "         173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 185, 186,\n",
       "         187, 190, 191, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203,\n",
       "         204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217,\n",
       "         218, 219, 220, 221, 222, 223, 224, 225, 226, 228, 229, 230, 231,\n",
       "         232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244,\n",
       "         245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 258,\n",
       "         259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271,\n",
       "         273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "         286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 297, 298, 299,\n",
       "         300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 313, 315,\n",
       "         316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328,\n",
       "         330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342,\n",
       "         343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
       "         356, 357, 358, 359, 360, 362, 363, 364, 366, 367, 368, 369, 370,\n",
       "         371, 372, 373, 374, 375, 376, 377, 379, 380, 381, 382, 383, 384,\n",
       "         385, 386, 387, 389, 390, 391, 393, 394, 395, 396, 397, 398, 399,\n",
       "         400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412,\n",
       "         414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426,\n",
       "         427, 428, 429, 430, 431, 432, 433, 435, 436, 437, 438, 439, 440,\n",
       "         441, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455,\n",
       "         456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468,\n",
       "         471, 472, 473, 474, 475, 476, 478, 479, 480, 481, 482, 483, 484,\n",
       "         486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498,\n",
       "         500, 501, 502, 503, 505, 506, 507, 508, 509, 510, 511, 512, 513,\n",
       "         514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 526, 527, 528,\n",
       "         529, 530, 531, 532, 533, 535, 536, 537, 538, 539, 540, 541, 542,\n",
       "         543, 545, 547, 548, 549, 550, 552, 553, 554, 555, 556, 557, 558,\n",
       "         559, 560, 561, 562, 563, 565, 566, 567, 568, 569, 570, 571, 572,\n",
       "         573, 575, 576, 578, 579, 580, 582, 583, 584, 585, 586, 587, 588,\n",
       "         589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601,\n",
       "         602, 606, 607, 608, 609, 610, 611, 612, 613, 614, 616, 617, 618,\n",
       "         619, 620, 621, 622, 623, 624, 625, 627, 628, 629, 630, 631, 632,\n",
       "         633, 635, 636, 637, 638, 640, 641, 642, 643, 644, 645, 646, 647,\n",
       "         648, 649, 650, 651, 652, 653, 654, 655, 656, 658, 660, 661, 662,\n",
       "         663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 674, 675, 677]),\n",
       "  array([  1,   3,   4,  17,  23,  27,  53,  65,  80, 103, 106, 108, 113,\n",
       "         139, 149, 151, 166, 184, 188, 189, 192, 195, 212, 227, 256, 272,\n",
       "         296, 309, 312, 314, 329, 361, 365, 378, 388, 392, 413, 434, 442,\n",
       "         443, 469, 470, 477, 485, 499, 504, 524, 525, 534, 544, 546, 551,\n",
       "         564, 574, 577, 581, 603, 604, 605, 615, 626, 634, 639, 657, 659,\n",
       "         673, 676])],\n",
       " [array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  21,  22,  23,  24,  25,  27,  28,\n",
       "          29,  30,  31,  32,  34,  35,  37,  38,  39,  40,  41,  42,  44,\n",
       "          46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,\n",
       "          59,  60,  61,  62,  63,  65,  66,  67,  69,  70,  71,  72,  73,\n",
       "          74,  75,  76,  78,  79,  80,  83,  84,  85,  86,  87,  88,  89,\n",
       "          90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
       "         103, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "         120, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 134,\n",
       "         135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148,\n",
       "         149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162,\n",
       "         163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 177,\n",
       "         178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191,\n",
       "         192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
       "         205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218,\n",
       "         219, 220, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232,\n",
       "         233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 244, 245, 246,\n",
       "         247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 261, 262,\n",
       "         263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 277,\n",
       "         278, 281, 282, 283, 284, 285, 286, 287, 289, 291, 292, 293, 294,\n",
       "         296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
       "         309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 321, 322,\n",
       "         323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
       "         336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348,\n",
       "         349, 350, 351, 352, 353, 354, 356, 357, 358, 359, 360, 361, 362,\n",
       "         363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376,\n",
       "         377, 378, 379, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
       "         392, 393, 394, 395, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
       "         406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418,\n",
       "         419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
       "         432, 433, 434, 435, 436, 437, 438, 439, 441, 442, 443, 445, 446,\n",
       "         447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459,\n",
       "         460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 472, 473,\n",
       "         474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
       "         487, 488, 489, 490, 491, 493, 494, 495, 496, 497, 498, 499, 500,\n",
       "         501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 514, 515,\n",
       "         516, 517, 518, 519, 520, 521, 523, 524, 525, 526, 527, 528, 529,\n",
       "         530, 531, 532, 534, 535, 536, 539, 540, 541, 542, 543, 544, 545,\n",
       "         546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 558, 559,\n",
       "         560, 561, 562, 563, 564, 565, 566, 567, 569, 570, 571, 572, 573,\n",
       "         574, 575, 576, 577, 578, 579, 581, 582, 584, 585, 586, 587, 589,\n",
       "         590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602,\n",
       "         603, 604, 605, 606, 607, 610, 611, 612, 613, 614, 615, 617, 618,\n",
       "         619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631,\n",
       "         632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644,\n",
       "         645, 646, 647, 648, 650, 651, 652, 653, 654, 655, 656, 657, 658,\n",
       "         659, 660, 661, 662, 663, 667, 668, 669, 670, 672, 673, 674, 676]),\n",
       "  array([ 11,  20,  26,  33,  36,  43,  45,  64,  68,  77,  81,  82, 104,\n",
       "         109, 118, 119, 121, 126, 142, 160, 164, 176, 187, 215, 221, 241,\n",
       "         253, 259, 260, 268, 276, 279, 280, 288, 290, 295, 320, 355, 372,\n",
       "         380, 391, 396, 440, 444, 471, 492, 512, 513, 522, 533, 537, 538,\n",
       "         557, 568, 580, 583, 588, 608, 609, 616, 649, 664, 665, 666, 671,\n",
       "         675, 677])]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BCAST_CV_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d28efdc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.save('BCAST_CV_indices.npy', BCAST_CV_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4881851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_indices = np.load(r\"D:\\NIH\\Mutational-Spectrum-master\\Data\\BCAST_CV_indices.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11fc795c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  14,\n",
       "                15,  17,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,\n",
       "                30,  31,  32,  33,  35,  36,  38,  40,  41,  42,  43,  44,  45,\n",
       "                46,  47,  49,  50,  51,  53,  54,  55,  57,  58,  61,  62,  64,\n",
       "                65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "                78,  80,  81,  82,  83,  84,  86,  87,  89,  90,  91,  92,  93,\n",
       "                94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,\n",
       "               107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n",
       "               121, 123, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136,\n",
       "               138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n",
       "               151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 163, 164, 165,\n",
       "               166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
       "               179, 180, 181, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193,\n",
       "               194, 195, 196, 197, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "               209, 210, 211, 212, 214, 215, 217, 218, 220, 221, 223, 225, 226,\n",
       "               227, 228, 229, 230, 231, 232, 234, 235, 236, 237, 238, 239, 240,\n",
       "               241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
       "               254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267,\n",
       "               268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
       "               281, 282, 283, 284, 285, 286, 288, 289, 290, 291, 292, 293, 294,\n",
       "               295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
       "               308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321,\n",
       "               322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334,\n",
       "               335, 336, 338, 339, 340, 342, 343, 344, 345, 346, 347, 348, 349,\n",
       "               350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 361, 363, 365,\n",
       "               366, 367, 368, 369, 370, 372, 373, 374, 376, 378, 379, 380, 381,\n",
       "               382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394,\n",
       "               396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408,\n",
       "               410, 411, 412, 413, 414, 416, 418, 419, 420, 422, 423, 424, 425,\n",
       "               426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438,\n",
       "               439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
       "               452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "               466, 467, 468, 469, 470, 471, 472, 473, 475, 476, 477, 478, 479,\n",
       "               480, 481, 482, 483, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "               494, 495, 496, 497, 499, 500, 501, 502, 503, 504, 505, 506, 507,\n",
       "               508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 519, 520, 522,\n",
       "               523, 524, 525, 526, 531, 532, 533, 534, 535, 536, 537, 538, 539,\n",
       "               540, 541, 542, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553,\n",
       "               554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566,\n",
       "               567, 568, 569, 570, 571, 572, 574, 575, 576, 577, 578, 579, 580,\n",
       "               581, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594,\n",
       "               595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607,\n",
       "               608, 609, 612, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "               624, 625, 626, 627, 629, 630, 631, 632, 633, 634, 635, 636, 637,\n",
       "               638, 639, 640, 641, 642, 643, 645, 646, 647, 648, 649, 651, 652,\n",
       "               653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665,\n",
       "               666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677])    ,\n",
       "        array([ 12,  13,  16,  18,  34,  37,  39,  48,  52,  56,  59,  60,  63,\n",
       "                79,  85,  88, 111, 122, 124, 132, 137, 158, 162, 182, 186, 198,\n",
       "               199, 213, 216, 219, 222, 224, 233, 264, 287, 318, 337, 341, 360,\n",
       "               362, 364, 371, 375, 377, 395, 409, 415, 417, 421, 455, 474, 484,\n",
       "               498, 518, 521, 527, 528, 529, 530, 543, 573, 582, 610, 611, 613,\n",
       "               628, 644, 650])                                                 ],\n",
       "       [array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "                13,  14,  15,  16,  17,  18,  19,  20,  22,  23,  24,  25,  26,\n",
       "                27,  28,  29,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
       "                41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
       "                54,  55,  56,  57,  58,  59,  60,  61,  63,  64,  65,  66,  67,\n",
       "                68,  70,  71,  72,  73,  74,  75,  77,  78,  79,  80,  81,  82,\n",
       "                83,  84,  85,  86,  87,  88,  89,  90,  91,  93,  94,  95,  96,\n",
       "                98,  99, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112,\n",
       "               113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "               126, 127, 128, 130, 131, 132, 135, 136, 137, 138, 139, 140, 141,\n",
       "               142, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158,\n",
       "               159, 160, 161, 162, 164, 166, 168, 169, 170, 171, 172, 173, 174,\n",
       "               175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187,\n",
       "               188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201,\n",
       "               202, 203, 204, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216,\n",
       "               217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229,\n",
       "               230, 231, 233, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244,\n",
       "               246, 247, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259,\n",
       "               260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
       "               274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287,\n",
       "               288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301,\n",
       "               303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316,\n",
       "               317, 318, 319, 320, 321, 323, 324, 327, 329, 330, 331, 332, 333,\n",
       "               334, 336, 337, 338, 340, 341, 342, 343, 344, 345, 346, 347, 348,\n",
       "               351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "               364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "               377, 378, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
       "               391, 392, 393, 394, 395, 396, 397, 399, 400, 401, 403, 404, 405,\n",
       "               406, 407, 408, 409, 410, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "               420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
       "               433, 434, 435, 436, 437, 440, 441, 442, 443, 444, 446, 447, 448,\n",
       "               449, 450, 451, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,\n",
       "               463, 464, 465, 466, 468, 469, 470, 471, 473, 474, 475, 476, 477,\n",
       "               478, 479, 480, 481, 483, 484, 485, 487, 488, 489, 490, 491, 492,\n",
       "               494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "               507, 508, 509, 510, 511, 512, 513, 515, 516, 517, 518, 519, 520,\n",
       "               521, 522, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534,\n",
       "               535, 536, 537, 538, 539, 541, 542, 543, 544, 545, 546, 548, 549,\n",
       "               550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562,\n",
       "               563, 564, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576,\n",
       "               577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 588, 589, 590,\n",
       "               592, 593, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605,\n",
       "               606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 619, 620,\n",
       "               621, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634,\n",
       "               635, 636, 637, 638, 639, 640, 642, 643, 644, 645, 646, 648, 649,\n",
       "               650, 651, 652, 653, 654, 655, 657, 658, 659, 661, 662, 663, 664,\n",
       "               665, 666, 667, 668, 670, 671, 672, 673, 674, 675, 676, 677])    ,\n",
       "        array([ 21,  30,  62,  69,  76,  92,  97, 100, 105, 129, 133, 134, 143,\n",
       "               144, 145, 157, 163, 165, 167, 197, 205, 211, 232, 234, 245, 252,\n",
       "               262, 281, 299, 302, 311, 322, 325, 326, 328, 335, 339, 349, 350,\n",
       "               379, 398, 402, 411, 438, 439, 445, 452, 467, 472, 482, 486, 493,\n",
       "               514, 523, 540, 547, 565, 587, 591, 594, 617, 618, 622, 641, 647,\n",
       "               656, 660, 669])                                                 ],\n",
       "       [array([  0,   1,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "                15,  16,  17,  18,  19,  20,  21,  23,  24,  25,  26,  27,  28,\n",
       "                29,  30,  31,  33,  34,  36,  37,  38,  39,  40,  41,  43,  44,\n",
       "                45,  46,  47,  48,  50,  51,  52,  53,  54,  55,  56,  57,  58,\n",
       "                59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  72,\n",
       "                74,  75,  76,  77,  79,  80,  81,  82,  83,  84,  85,  86,  87,\n",
       "                88,  89,  90,  91,  92,  94,  95,  96,  97,  99, 100, 101, 102,\n",
       "               103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
       "               116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "               130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143,\n",
       "               144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157,\n",
       "               158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 171,\n",
       "               172, 173, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186,\n",
       "               187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199,\n",
       "               200, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 215,\n",
       "               216, 217, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230,\n",
       "               231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244,\n",
       "               245, 246, 248, 249, 250, 252, 253, 254, 256, 257, 258, 259, 260,\n",
       "               261, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 273, 274,\n",
       "               275, 276, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 290,\n",
       "               291, 292, 293, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305,\n",
       "               306, 307, 308, 309, 311, 312, 314, 315, 316, 317, 318, 319, 320,\n",
       "               321, 322, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335,\n",
       "               336, 337, 338, 339, 340, 341, 343, 344, 345, 346, 347, 348, 349,\n",
       "               350, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363,\n",
       "               364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "               377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "               390, 391, 392, 393, 394, 395, 396, 398, 399, 400, 402, 403, 404,\n",
       "               405, 406, 407, 408, 409, 410, 411, 413, 414, 415, 416, 417, 418,\n",
       "               419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
       "               432, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445,\n",
       "               446, 447, 448, 449, 450, 452, 453, 454, 455, 457, 460, 461, 462,\n",
       "               464, 465, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477,\n",
       "               478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490,\n",
       "               491, 492, 493, 494, 495, 497, 498, 499, 500, 501, 502, 503, 504,\n",
       "               505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 517, 518,\n",
       "               520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "               533, 534, 535, 536, 537, 538, 539, 540, 542, 543, 544, 545, 546,\n",
       "               547, 548, 549, 550, 551, 552, 553, 554, 555, 557, 558, 559, 560,\n",
       "               562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574,\n",
       "               576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588,\n",
       "               590, 591, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604,\n",
       "               605, 606, 607, 608, 609, 610, 611, 612, 613, 615, 616, 617, 618,\n",
       "               619, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 633, 634,\n",
       "               635, 636, 637, 638, 639, 641, 643, 644, 645, 646, 647, 649, 650,\n",
       "               651, 652, 653, 655, 656, 657, 658, 659, 660, 661, 663, 664, 665,\n",
       "               666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677])    ,\n",
       "        array([  2,  14,  22,  32,  35,  42,  49,  71,  73,  78,  93,  98, 117,\n",
       "               136, 150, 169, 174, 179, 201, 206, 214, 218, 228, 239, 247, 251,\n",
       "               255, 266, 277, 278, 289, 294, 304, 310, 313, 323, 332, 342, 359,\n",
       "               397, 401, 412, 433, 451, 456, 458, 459, 463, 466, 496, 516, 519,\n",
       "               541, 556, 561, 575, 589, 592, 593, 614, 620, 621, 632, 640, 642,\n",
       "               648, 654, 662])                                                 ],\n",
       "       [array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,\n",
       "                14,  16,  17,  18,  19,  20,  21,  22,  23,  25,  26,  27,  28,\n",
       "                29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "                42,  43,  44,  45,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
       "                57,  58,  59,  60,  62,  63,  64,  65,  66,  67,  68,  69,  71,\n",
       "                72,  73,  74,  76,  77,  78,  79,  80,  81,  82,  83,  85,  86,\n",
       "                87,  88,  89,  90,  91,  92,  93,  94,  96,  97,  98,  99, 100,\n",
       "               101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114,\n",
       "               115, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129,\n",
       "               130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "               144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157,\n",
       "               158, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171,\n",
       "               172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185,\n",
       "               186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
       "               199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
       "               213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 227,\n",
       "               228, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243,\n",
       "               244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256,\n",
       "               257, 258, 259, 260, 261, 262, 263, 264, 266, 267, 268, 269, 271,\n",
       "               272, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "               286, 287, 288, 289, 290, 291, 293, 294, 295, 296, 298, 299, 300,\n",
       "               302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314,\n",
       "               315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327,\n",
       "               328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 341, 342,\n",
       "               345, 346, 347, 348, 349, 350, 353, 354, 355, 358, 359, 360, 361,\n",
       "               362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374,\n",
       "               375, 376, 377, 378, 379, 380, 381, 383, 384, 385, 386, 387, 388,\n",
       "               390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "               403, 404, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417,\n",
       "               418, 419, 420, 421, 422, 423, 424, 425, 428, 431, 433, 434, 435,\n",
       "               436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 447, 448, 449,\n",
       "               450, 451, 452, 453, 454, 455, 456, 458, 459, 460, 461, 462, 463,\n",
       "               464, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477,\n",
       "               478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 490, 491,\n",
       "               492, 493, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505,\n",
       "               506, 507, 508, 509, 510, 512, 513, 514, 515, 516, 518, 519, 521,\n",
       "               522, 523, 524, 525, 527, 528, 529, 530, 531, 532, 533, 534, 535,\n",
       "               536, 537, 538, 539, 540, 541, 543, 544, 545, 546, 547, 548, 550,\n",
       "               551, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
       "               565, 566, 567, 568, 569, 570, 573, 574, 575, 577, 578, 580, 581,\n",
       "               582, 583, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595,\n",
       "               596, 598, 600, 601, 602, 603, 604, 605, 606, 608, 609, 610, 611,\n",
       "               613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 625, 626,\n",
       "               627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639,\n",
       "               640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652,\n",
       "               653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665,\n",
       "               666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677])    ,\n",
       "        array([ 10,  15,  24,  46,  47,  61,  70,  75,  84,  95, 110, 116, 123,\n",
       "               135, 155, 159, 177, 203, 225, 226, 229, 230, 238, 265, 270, 273,\n",
       "               292, 297, 301, 338, 340, 343, 344, 351, 352, 356, 357, 382, 389,\n",
       "               405, 406, 426, 427, 429, 430, 432, 446, 457, 465, 489, 494, 511,\n",
       "               517, 520, 526, 542, 549, 552, 571, 572, 576, 579, 584, 597, 599,\n",
       "               607, 612, 624])                                                 ],\n",
       "       [array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "                14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  26,  27,\n",
       "                28,  29,  30,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "                42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,\n",
       "                56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  67,  68,  69,\n",
       "                70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
       "                84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  97,\n",
       "                98, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112,\n",
       "               113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
       "               127, 129, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143,\n",
       "               144, 145, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158,\n",
       "               159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172,\n",
       "               173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186,\n",
       "               187, 188, 189, 192, 193, 194, 195, 197, 198, 199, 201, 202, 203,\n",
       "               205, 206, 208, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
       "               220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232,\n",
       "               233, 234, 235, 236, 237, 238, 239, 240, 241, 244, 245, 247, 248,\n",
       "               249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262,\n",
       "               263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275,\n",
       "               276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 287, 288, 289,\n",
       "               290, 292, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304,\n",
       "               305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317,\n",
       "               318, 319, 320, 321, 322, 323, 325, 326, 327, 328, 329, 330, 332,\n",
       "               334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346,\n",
       "               347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359,\n",
       "               360, 361, 362, 363, 364, 365, 366, 367, 369, 370, 371, 372, 373,\n",
       "               374, 375, 377, 378, 379, 380, 382, 383, 384, 385, 387, 388, 389,\n",
       "               390, 391, 392, 393, 394, 395, 396, 397, 398, 401, 402, 403, 404,\n",
       "               405, 406, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "               421, 422, 423, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n",
       "               436, 438, 439, 440, 442, 443, 444, 445, 446, 447, 448, 449, 451,\n",
       "               452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 463, 464, 465,\n",
       "               466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479,\n",
       "               480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 492, 493, 494,\n",
       "               495, 496, 498, 499, 500, 501, 503, 504, 505, 506, 507, 509, 510,\n",
       "               511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523,\n",
       "               524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536,\n",
       "               537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549,\n",
       "               550, 551, 552, 553, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n",
       "               564, 565, 566, 567, 568, 570, 571, 572, 573, 574, 575, 576, 577,\n",
       "               578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 591,\n",
       "               592, 593, 594, 596, 597, 598, 599, 600, 601, 603, 604, 605, 606,\n",
       "               607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619,\n",
       "               620, 621, 622, 623, 624, 625, 626, 628, 629, 631, 632, 633, 634,\n",
       "               635, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
       "               650, 651, 652, 653, 654, 655, 656, 657, 659, 660, 662, 664, 665,\n",
       "               666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677])    ,\n",
       "        array([  0,  25,  31,  55,  66,  83,  96,  99, 107, 114, 128, 130, 131,\n",
       "               141, 146, 153, 161, 185, 190, 191, 196, 200, 204, 207, 209, 242,\n",
       "               243, 246, 257, 283, 291, 298, 324, 331, 333, 368, 376, 381, 386,\n",
       "               399, 400, 407, 408, 420, 424, 425, 437, 441, 450, 462, 475, 490,\n",
       "               491, 497, 502, 508, 554, 569, 590, 595, 602, 627, 630, 636, 637,\n",
       "               658, 661, 663])                                                 ],\n",
       "       [array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "                13,  14,  15,  16,  17,  18,  20,  21,  22,  23,  24,  25,  26,\n",
       "                27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "                41,  42,  43,  45,  46,  47,  48,  49,  50,  52,  53,  54,  55,\n",
       "                56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
       "                69,  70,  71,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
       "                83,  84,  85,  86,  87,  88,  89,  90,  92,  93,  95,  96,  97,\n",
       "                98,  99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "               113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "               126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139,\n",
       "               140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 153, 155, 156,\n",
       "               157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "               171, 172, 173, 174, 175, 176, 177, 178, 179, 182, 184, 185, 186,\n",
       "               187, 188, 189, 190, 191, 192, 193, 195, 196, 197, 198, 199, 200,\n",
       "               201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214,\n",
       "               215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229,\n",
       "               230, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244,\n",
       "               245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 256, 257, 258,\n",
       "               259, 260, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "               273, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 287,\n",
       "               288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300,\n",
       "               301, 302, 304, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
       "               317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330,\n",
       "               331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343,\n",
       "               344, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358,\n",
       "               359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 371, 372,\n",
       "               373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 385, 386, 387,\n",
       "               388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400,\n",
       "               401, 402, 403, 404, 405, 406, 407, 408, 409, 411, 412, 413, 415,\n",
       "               416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "               429, 430, 432, 433, 434, 435, 437, 438, 439, 440, 441, 442, 443,\n",
       "               444, 445, 446, 448, 450, 451, 452, 453, 454, 455, 456, 457, 458,\n",
       "               459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471,\n",
       "               472, 473, 474, 475, 476, 477, 479, 480, 482, 484, 485, 486, 487,\n",
       "               488, 489, 490, 491, 492, 493, 494, 496, 497, 498, 499, 500, 501,\n",
       "               502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514,\n",
       "               515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527,\n",
       "               528, 529, 530, 532, 533, 534, 535, 536, 537, 538, 540, 541, 542,\n",
       "               543, 544, 545, 546, 547, 548, 549, 551, 552, 554, 555, 556, 557,\n",
       "               558, 559, 560, 561, 563, 564, 565, 567, 568, 569, 570, 571, 572,\n",
       "               573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 587,\n",
       "               588, 589, 590, 591, 592, 593, 594, 595, 597, 598, 599, 601, 602,\n",
       "               603, 604, 605, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,\n",
       "               617, 618, 620, 621, 622, 623, 624, 626, 627, 628, 630, 632, 634,\n",
       "               636, 637, 638, 639, 640, 641, 642, 643, 644, 646, 647, 648, 649,\n",
       "               650, 653, 654, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665,\n",
       "               666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677])    ,\n",
       "        array([ 19,  40,  44,  51,  72,  91,  94, 102, 112, 138, 147, 148, 152,\n",
       "               154, 170, 180, 181, 183, 194, 210, 217, 220, 231, 240, 254, 261,\n",
       "               274, 286, 303, 305, 306, 327, 345, 346, 370, 383, 384, 410, 414,\n",
       "               431, 436, 447, 449, 478, 481, 483, 495, 531, 539, 550, 553, 562,\n",
       "               566, 585, 586, 596, 600, 606, 619, 625, 629, 631, 633, 635, 645,\n",
       "               651, 652, 655])                                                 ],\n",
       "       [array([  0,   1,   2,   3,   4,   7,   8,  10,  11,  12,  13,  14,  15,\n",
       "                16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
       "                29,  30,  31,  32,  33,  34,  35,  36,  37,  39,  40,  42,  43,\n",
       "                44,  45,  46,  47,  48,  49,  51,  52,  53,  54,  55,  56,  57,\n",
       "                59,  60,  61,  62,  63,  64,  65,  66,  68,  69,  70,  71,  72,\n",
       "                73,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  88,\n",
       "                91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 102, 103, 104,\n",
       "               105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118,\n",
       "               119, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133,\n",
       "               134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146,\n",
       "               147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160,\n",
       "               161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 174, 175,\n",
       "               176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188,\n",
       "               189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 203,\n",
       "               204, 205, 206, 207, 209, 210, 211, 212, 213, 214, 215, 216, 217,\n",
       "               218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231,\n",
       "               232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "               247, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262,\n",
       "               263, 264, 265, 266, 267, 268, 269, 270, 272, 273, 274, 275, 276,\n",
       "               277, 278, 279, 280, 281, 282, 283, 285, 286, 287, 288, 289, 290,\n",
       "               291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303,\n",
       "               304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 318,\n",
       "               320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n",
       "               334, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 348,\n",
       "               349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361,\n",
       "               362, 364, 365, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "               378, 379, 380, 381, 382, 383, 384, 386, 388, 389, 390, 391, 392,\n",
       "               393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
       "               406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 420,\n",
       "               421, 423, 424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 436,\n",
       "               437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 449, 450,\n",
       "               451, 452, 453, 455, 456, 457, 458, 459, 460, 461, 462, 463, 465,\n",
       "               466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
       "               480, 481, 482, 483, 484, 485, 486, 488, 489, 490, 491, 492, 493,\n",
       "               494, 495, 496, 497, 498, 499, 501, 502, 503, 504, 505, 506, 508,\n",
       "               509, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522,\n",
       "               523, 524, 525, 526, 527, 528, 529, 530, 531, 533, 534, 535, 537,\n",
       "               538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
       "               552, 553, 554, 555, 556, 557, 561, 562, 563, 564, 565, 566, 567,\n",
       "               568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 579, 580, 581,\n",
       "               582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594,\n",
       "               595, 596, 597, 599, 600, 602, 603, 604, 605, 606, 607, 608, 609,\n",
       "               610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622,\n",
       "               623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635,\n",
       "               636, 637, 639, 640, 641, 642, 644, 645, 647, 648, 649, 650, 651,\n",
       "               652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664,\n",
       "               665, 666, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677])    ,\n",
       "        array([  5,   6,   9,  38,  41,  50,  58,  67,  74,  86,  87,  89,  90,\n",
       "               101, 115, 120, 127, 156, 168, 173, 193, 202, 208, 223, 235, 237,\n",
       "               248, 249, 250, 271, 284, 316, 317, 319, 321, 336, 347, 363, 366,\n",
       "               367, 385, 387, 418, 419, 422, 428, 435, 448, 454, 464, 479, 487,\n",
       "               500, 507, 510, 532, 536, 548, 558, 559, 560, 578, 598, 601, 638,\n",
       "               643, 646, 667])                                                 ],\n",
       "       [array([  0,   1,   2,   3,   4,   5,   6,   9,  10,  11,  12,  13,  14,\n",
       "                15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "                30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "                43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  55,  56,\n",
       "                58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
       "                71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "                84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,\n",
       "                97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
       "               110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122,\n",
       "               123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136,\n",
       "               137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n",
       "               151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163,\n",
       "               164, 165, 166, 167, 168, 169, 170, 173, 174, 176, 177, 179, 180,\n",
       "               181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193,\n",
       "               194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206,\n",
       "               207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
       "               220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232,\n",
       "               233, 234, 235, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247,\n",
       "               248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261,\n",
       "               262, 264, 265, 266, 268, 270, 271, 272, 273, 274, 276, 277, 278,\n",
       "               279, 280, 281, 283, 284, 286, 287, 288, 289, 290, 291, 292, 294,\n",
       "               295, 296, 297, 298, 299, 301, 302, 303, 304, 305, 306, 309, 310,\n",
       "               311, 312, 313, 314, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "               325, 326, 327, 328, 329, 331, 332, 333, 335, 336, 337, 338, 339,\n",
       "               340, 341, 342, 343, 344, 345, 346, 347, 349, 350, 351, 352, 355,\n",
       "               356, 357, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 370,\n",
       "               371, 372, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n",
       "               386, 387, 388, 389, 391, 392, 395, 396, 397, 398, 399, 400, 401,\n",
       "               402, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 417,\n",
       "               418, 419, 420, 421, 422, 424, 425, 426, 427, 428, 429, 430, 431,\n",
       "               432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444,\n",
       "               445, 446, 447, 448, 449, 450, 451, 452, 454, 455, 456, 457, 458,\n",
       "               459, 462, 463, 464, 465, 466, 467, 469, 470, 471, 472, 474, 475,\n",
       "               477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 489, 490, 491,\n",
       "               492, 493, 494, 495, 496, 497, 498, 499, 500, 502, 504, 507, 508,\n",
       "               510, 511, 512, 513, 514, 516, 517, 518, 519, 520, 521, 522, 523,\n",
       "               524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 536, 537,\n",
       "               538, 539, 540, 541, 542, 543, 544, 546, 547, 548, 549, 550, 551,\n",
       "               552, 553, 554, 556, 557, 558, 559, 560, 561, 562, 564, 565, 566,\n",
       "               568, 569, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581,\n",
       "               582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594,\n",
       "               595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607,\n",
       "               608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620,\n",
       "               621, 622, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634,\n",
       "               635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647,\n",
       "               648, 649, 650, 651, 652, 654, 655, 656, 657, 658, 659, 660, 661,\n",
       "               662, 663, 664, 665, 666, 667, 669, 671, 673, 675, 676, 677])    ,\n",
       "        array([  7,   8,  28,  29,  54,  57, 125, 140, 171, 172, 175, 178, 236,\n",
       "               244, 258, 263, 267, 269, 275, 282, 285, 293, 300, 307, 308, 315,\n",
       "               330, 334, 348, 353, 354, 358, 369, 373, 374, 390, 393, 394, 403,\n",
       "               404, 416, 423, 453, 460, 461, 468, 473, 476, 480, 488, 501, 503,\n",
       "               505, 506, 509, 515, 535, 545, 555, 563, 567, 570, 623, 653, 668,\n",
       "               670, 672, 674])                                                 ],\n",
       "       [array([  0,   2,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,\n",
       "                16,  18,  19,  20,  21,  22,  24,  25,  26,  28,  29,  30,  31,\n",
       "                32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,\n",
       "                45,  46,  47,  48,  49,  50,  51,  52,  54,  55,  56,  57,  58,\n",
       "                59,  60,  61,  62,  63,  64,  66,  67,  68,  69,  70,  71,  72,\n",
       "                73,  74,  75,  76,  77,  78,  79,  81,  82,  83,  84,  85,  86,\n",
       "                87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99,\n",
       "               100, 101, 102, 104, 105, 107, 109, 110, 111, 112, 114, 115, 116,\n",
       "               117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "               130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143,\n",
       "               144, 145, 146, 147, 148, 150, 152, 153, 154, 155, 156, 157, 158,\n",
       "               159, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172,\n",
       "               173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 185, 186,\n",
       "               187, 190, 191, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203,\n",
       "               204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217,\n",
       "               218, 219, 220, 221, 222, 223, 224, 225, 226, 228, 229, 230, 231,\n",
       "               232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244,\n",
       "               245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 258,\n",
       "               259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271,\n",
       "               273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "               286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 297, 298, 299,\n",
       "               300, 301, 302, 303, 304, 305, 306, 307, 308, 310, 311, 313, 315,\n",
       "               316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328,\n",
       "               330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342,\n",
       "               343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
       "               356, 357, 358, 359, 360, 362, 363, 364, 366, 367, 368, 369, 370,\n",
       "               371, 372, 373, 374, 375, 376, 377, 379, 380, 381, 382, 383, 384,\n",
       "               385, 386, 387, 389, 390, 391, 393, 394, 395, 396, 397, 398, 399,\n",
       "               400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412,\n",
       "               414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426,\n",
       "               427, 428, 429, 430, 431, 432, 433, 435, 436, 437, 438, 439, 440,\n",
       "               441, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455,\n",
       "               456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468,\n",
       "               471, 472, 473, 474, 475, 476, 478, 479, 480, 481, 482, 483, 484,\n",
       "               486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498,\n",
       "               500, 501, 502, 503, 505, 506, 507, 508, 509, 510, 511, 512, 513,\n",
       "               514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 526, 527, 528,\n",
       "               529, 530, 531, 532, 533, 535, 536, 537, 538, 539, 540, 541, 542,\n",
       "               543, 545, 547, 548, 549, 550, 552, 553, 554, 555, 556, 557, 558,\n",
       "               559, 560, 561, 562, 563, 565, 566, 567, 568, 569, 570, 571, 572,\n",
       "               573, 575, 576, 578, 579, 580, 582, 583, 584, 585, 586, 587, 588,\n",
       "               589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601,\n",
       "               602, 606, 607, 608, 609, 610, 611, 612, 613, 614, 616, 617, 618,\n",
       "               619, 620, 621, 622, 623, 624, 625, 627, 628, 629, 630, 631, 632,\n",
       "               633, 635, 636, 637, 638, 640, 641, 642, 643, 644, 645, 646, 647,\n",
       "               648, 649, 650, 651, 652, 653, 654, 655, 656, 658, 660, 661, 662,\n",
       "               663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 674, 675, 677]),\n",
       "        array([  1,   3,   4,  17,  23,  27,  53,  65,  80, 103, 106, 108, 113,\n",
       "               139, 149, 151, 166, 184, 188, 189, 192, 195, 212, 227, 256, 272,\n",
       "               296, 309, 312, 314, 329, 361, 365, 378, 388, 392, 413, 434, 442,\n",
       "               443, 469, 470, 477, 485, 499, 504, 524, 525, 534, 544, 546, 551,\n",
       "               564, 574, 577, 581, 603, 604, 605, 615, 626, 634, 639, 657, 659,\n",
       "               673, 676])                                                      ],\n",
       "       [array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  12,  13,\n",
       "                14,  15,  16,  17,  18,  19,  21,  22,  23,  24,  25,  27,  28,\n",
       "                29,  30,  31,  32,  34,  35,  37,  38,  39,  40,  41,  42,  44,\n",
       "                46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,\n",
       "                59,  60,  61,  62,  63,  65,  66,  67,  69,  70,  71,  72,  73,\n",
       "                74,  75,  76,  78,  79,  80,  83,  84,  85,  86,  87,  88,  89,\n",
       "                90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
       "               103, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "               120, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 134,\n",
       "               135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148,\n",
       "               149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162,\n",
       "               163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 177,\n",
       "               178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191,\n",
       "               192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
       "               205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218,\n",
       "               219, 220, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232,\n",
       "               233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 244, 245, 246,\n",
       "               247, 248, 249, 250, 251, 252, 254, 255, 256, 257, 258, 261, 262,\n",
       "               263, 264, 265, 266, 267, 269, 270, 271, 272, 273, 274, 275, 277,\n",
       "               278, 281, 282, 283, 284, 285, 286, 287, 289, 291, 292, 293, 294,\n",
       "               296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
       "               309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 321, 322,\n",
       "               323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
       "               336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348,\n",
       "               349, 350, 351, 352, 353, 354, 356, 357, 358, 359, 360, 361, 362,\n",
       "               363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376,\n",
       "               377, 378, 379, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
       "               392, 393, 394, 395, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
       "               406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418,\n",
       "               419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
       "               432, 433, 434, 435, 436, 437, 438, 439, 441, 442, 443, 445, 446,\n",
       "               447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459,\n",
       "               460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 472, 473,\n",
       "               474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
       "               487, 488, 489, 490, 491, 493, 494, 495, 496, 497, 498, 499, 500,\n",
       "               501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 514, 515,\n",
       "               516, 517, 518, 519, 520, 521, 523, 524, 525, 526, 527, 528, 529,\n",
       "               530, 531, 532, 534, 535, 536, 539, 540, 541, 542, 543, 544, 545,\n",
       "               546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 558, 559,\n",
       "               560, 561, 562, 563, 564, 565, 566, 567, 569, 570, 571, 572, 573,\n",
       "               574, 575, 576, 577, 578, 579, 581, 582, 584, 585, 586, 587, 589,\n",
       "               590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602,\n",
       "               603, 604, 605, 606, 607, 610, 611, 612, 613, 614, 615, 617, 618,\n",
       "               619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631,\n",
       "               632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644,\n",
       "               645, 646, 647, 648, 650, 651, 652, 653, 654, 655, 656, 657, 658,\n",
       "               659, 660, 661, 662, 663, 667, 668, 669, 670, 672, 673, 674, 676]),\n",
       "        array([ 11,  20,  26,  33,  36,  43,  45,  64,  68,  77,  81,  82, 104,\n",
       "               109, 118, 119, 121, 126, 142, 160, 164, 176, 187, 215, 221, 241,\n",
       "               253, 259, 260, 268, 276, 279, 280, 288, 290, 295, 320, 355, 372,\n",
       "               380, 391, 396, 440, 444, 471, 492, 512, 513, 522, 533, 537, 538,\n",
       "               557, 568, 580, 583, 588, 608, 609, 616, 649, 664, 665, 666, 671,\n",
       "               675, 677])                                                      ]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ac032a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-14 12:50:41,330]\u001b[0m A new study created in memory with name: no-name-5b2ef980-61ea-473b-80a8-6045046a062e\u001b[0m\n",
      "\u001b[33m[W 2022-07-14 12:50:42,062]\u001b[0m Trial 0 failed because of the following error: InvalidArgumentError()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\2296759501.py\", line 28, in compute_mlp_performance\n",
      "    model.fit(X_train, y_train,\n",
      "  File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 54, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\n",
      "\n",
      "Detected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 86, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 972, in launch_instance\n",
      "      app.start()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n",
      "      self.io_loop.start()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n",
      "      self._run_once()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n",
      "      handle._run()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 504, in dispatch_queue\n",
      "      await self.process_one()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 493, in process_one\n",
      "      await dispatch(*args)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "      await result\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 724, in execute_request\n",
      "      reply_content = await reply_content\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n",
      "      res = shell.run_cell(\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n",
      "      return super().run_cell(*args, **kwargs)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in run_cell\n",
      "      result = self._run_cell(\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2935, in _run_cell\n",
      "      return runner(coro)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3134, in run_cell_async\n",
      "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n",
      "      if await self.run_code(code, result, async_=asy):\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\3244150248.py\", line 2, in <cell line: 2>\n",
      "      study_is_sig3.optimize(compute_mlp_performance, n_trials=300)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\study.py\", line 400, in optimize\n",
      "      _optimize(\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 66, in _optimize\n",
      "      _optimize_sequential(\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 163, in _optimize_sequential\n",
      "      trial = _run_trial(study, func, catch)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n",
      "      value_or_values = func(trial)\n",
      "    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\2296759501.py\", line 28, in compute_mlp_performance\n",
      "      model.fit(X_train, y_train,\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n",
      "      tmp_logs = self.train_function(iterator)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n",
      "      return step_function(self, iterator)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n",
      "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n",
      "      outputs = model.train_step(data)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 894, in train_step\n",
      "      return self.compute_metrics(x, y, y_pred, sample_weight)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 987, in compute_metrics\n",
      "      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 501, in update_state\n",
      "      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n",
      "      update_op = update_state_fn(*args, **kwargs)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n",
      "      return ag_update_state(*args, **kwargs)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\metrics.py\", line 1759, in update_state\n",
      "      return metrics_utils.update_confusion_matrix_variables(\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 602, in update_confusion_matrix_variables\n",
      "      tf.debugging.assert_greater_equal(\n",
      "Node: 'assert_greater_equal/Assert/AssertGuard/Assert'\n",
      "Detected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "      return _run_code(code, main_globals, None,\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 86, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "      app.launch_new_instance()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 972, in launch_instance\n",
      "      app.start()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n",
      "      self.io_loop.start()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "      self.asyncio_loop.run_forever()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n",
      "      self._run_once()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n",
      "      handle._run()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "      self._context.run(self._callback, *self._args)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 504, in dispatch_queue\n",
      "      await self.process_one()\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 493, in process_one\n",
      "      await dispatch(*args)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "      await result\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 724, in execute_request\n",
      "      reply_content = await reply_content\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n",
      "      res = shell.run_cell(\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n",
      "      return super().run_cell(*args, **kwargs)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in run_cell\n",
      "      result = self._run_cell(\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2935, in _run_cell\n",
      "      return runner(coro)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "      coro.send(None)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3134, in run_cell_async\n",
      "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n",
      "      if await self.run_code(code, result, async_=asy):\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n",
      "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\3244150248.py\", line 2, in <cell line: 2>\n",
      "      study_is_sig3.optimize(compute_mlp_performance, n_trials=300)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\study.py\", line 400, in optimize\n",
      "      _optimize(\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 66, in _optimize\n",
      "      _optimize_sequential(\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 163, in _optimize_sequential\n",
      "      trial = _run_trial(study, func, catch)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n",
      "      value_or_values = func(trial)\n",
      "    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\2296759501.py\", line 28, in compute_mlp_performance\n",
      "      model.fit(X_train, y_train,\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n",
      "      return fn(*args, **kwargs)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n",
      "      tmp_logs = self.train_function(iterator)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n",
      "      return step_function(self, iterator)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n",
      "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n",
      "      outputs = model.train_step(data)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 894, in train_step\n",
      "      return self.compute_metrics(x, y, y_pred, sample_weight)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 987, in compute_metrics\n",
      "      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 501, in update_state\n",
      "      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n",
      "      update_op = update_state_fn(*args, **kwargs)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n",
      "      return ag_update_state(*args, **kwargs)\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\metrics.py\", line 1759, in update_state\n",
      "      return metrics_utils.update_confusion_matrix_variables(\n",
      "    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 602, in update_confusion_matrix_variables\n",
      "      tf.debugging.assert_greater_equal(\n",
      "Node: 'assert_greater_equal/Assert/AssertGuard/Assert'\n",
      "2 root error(s) found.\n",
      "  (0) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential_4/dense_14/Sigmoid:0) = ] [[0.49993217][0.49993217][0.49993217]...] [y (Cast_3/x:0) = ] [0]\n",
      "\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n",
      "\t [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_43]]\n",
      "  (1) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential_4/dense_14/Sigmoid:0) = ] [[0.49993217][0.49993217][0.49993217]...] [y (Cast_3/x:0) = ] [0]\n",
      "\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_6181]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 972, in launch_instance\n      app.start()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\3244150248.py\", line 2, in <cell line: 2>\n      study_is_sig3.optimize(compute_mlp_performance, n_trials=300)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\study.py\", line 400, in optimize\n      _optimize(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 66, in _optimize\n      _optimize_sequential(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 163, in _optimize_sequential\n      trial = _run_trial(study, func, catch)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n      value_or_values = func(trial)\n    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\2296759501.py\", line 28, in compute_mlp_performance\n      model.fit(X_train, y_train,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 894, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 987, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 501, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\metrics.py\", line 1759, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.debugging.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 972, in launch_instance\n      app.start()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\3244150248.py\", line 2, in <cell line: 2>\n      study_is_sig3.optimize(compute_mlp_performance, n_trials=300)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\study.py\", line 400, in optimize\n      _optimize(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 66, in _optimize\n      _optimize_sequential(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 163, in _optimize_sequential\n      trial = _run_trial(study, func, catch)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n      value_or_values = func(trial)\n    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\2296759501.py\", line 28, in compute_mlp_performance\n      model.fit(X_train, y_train,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 894, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 987, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 501, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\metrics.py\", line 1759, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.debugging.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential_4/dense_14/Sigmoid:0) = ] [[0.49993217][0.49993217][0.49993217]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n\t [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_43]]\n  (1) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential_4/dense_14/Sigmoid:0) = ] [[0.49993217][0.49993217][0.49993217]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_6181]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m study_is_sig3 \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mstudy_is_sig3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_mlp_performance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis feature will be removed in v4.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py:264\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch):\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mcompute_mlp_performance\u001b[1;34m(trial, input_shape, data_dir, n_folds, class_weight_0, class_weight_1, num_samples, y_col)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# build model and ensure that parameters passed in are within the normal range\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# if we don't type cast as integers, bayesian optimizer will guess float values\u001b[39;00m\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m build_mlp_model(input_shape, \n\u001b[0;32m     21\u001b[0m                         trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_hidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m), \n\u001b[0;32m     22\u001b[0m                         trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_hidden_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m300\u001b[39m), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m                         l2_kernel \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2_kernel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5e-1\u001b[39m),\n\u001b[0;32m     27\u001b[0m                         l2_bias \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2_bias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5e-1\u001b[39m))\n\u001b[1;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m          \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_float\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass_weight_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_float\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass_weight_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_auc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n\u001b[0;32m     35\u001b[0m y_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 972, in launch_instance\n      app.start()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\3244150248.py\", line 2, in <cell line: 2>\n      study_is_sig3.optimize(compute_mlp_performance, n_trials=300)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\study.py\", line 400, in optimize\n      _optimize(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 66, in _optimize\n      _optimize_sequential(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 163, in _optimize_sequential\n      trial = _run_trial(study, func, catch)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n      value_or_values = func(trial)\n    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\2296759501.py\", line 28, in compute_mlp_performance\n      model.fit(X_train, y_train,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 894, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 987, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 501, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\metrics.py\", line 1759, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.debugging.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\nDetected at node 'assert_greater_equal/Assert/AssertGuard/Assert' defined at (most recent call last):\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 972, in launch_instance\n      app.start()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\3244150248.py\", line 2, in <cell line: 2>\n      study_is_sig3.optimize(compute_mlp_performance, n_trials=300)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\study.py\", line 400, in optimize\n      _optimize(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 66, in _optimize\n      _optimize_sequential(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 163, in _optimize_sequential\n      trial = _run_trial(study, func, catch)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n      value_or_values = func(trial)\n    File \"C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_6800\\2296759501.py\", line 28, in compute_mlp_performance\n      model.fit(X_train, y_train,\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 894, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py\", line 987, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 501, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 70, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\metrics\\metrics.py\", line 1759, in update_state\n      return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\aaron\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 602, in update_confusion_matrix_variables\n      tf.debugging.assert_greater_equal(\nNode: 'assert_greater_equal/Assert/AssertGuard/Assert'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential_4/dense_14/Sigmoid:0) = ] [[0.49993217][0.49993217][0.49993217]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n\t [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_43]]\n  (1) INVALID_ARGUMENT:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (sequential_4/dense_14/Sigmoid:0) = ] [[0.49993217][0.49993217][0.49993217]...] [y (Cast_3/x:0) = ] [0]\n\t [[{{node assert_greater_equal/Assert/AssertGuard/Assert}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_6181]"
     ]
    }
   ],
   "source": [
    "study_is_sig3 = optuna.create_study(direction='maximize')\n",
    "study_is_sig3.optimize(compute_mlp_performance, n_trials=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd124228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-06 10:39:04,871]\u001b[0m Trial 150 finished with value: 0.6679738759994507 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 179, 'activation': 'relu', 'learning_rate': 0.004618475069081576, 'weight_decay': 0.029733993392913993, 'l2_kernel': 0.020818081350416715, 'l2_bias': 0.4933002261074192, 'class_weight_0': 3.9720038812960907, 'class_weight_1': 2.5278886719603872}. Best is trial 41 with value: 0.6877726316452026.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:39:17,529]\u001b[0m Trial 151 finished with value: 0.6476271152496338 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 184, 'activation': 'relu', 'learning_rate': 0.005043740187310367, 'weight_decay': 0.02269038397548642, 'l2_kernel': 0.02192158583791648, 'l2_bias': 0.49999111268295743, 'class_weight_0': 3.9047414194656396, 'class_weight_1': 2.502705394404022}. Best is trial 41 with value: 0.6877726316452026.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:39:28,525]\u001b[0m Trial 152 finished with value: 0.6519607901573181 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 167, 'activation': 'relu', 'learning_rate': 0.009824330914708966, 'weight_decay': 0.03608707323741037, 'l2_kernel': 0.02164574049040428, 'l2_bias': 0.4879747666195251, 'class_weight_0': 3.6684354388134026, 'class_weight_1': 2.8197955973424675}. Best is trial 41 with value: 0.6877726316452026.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:39:40,885]\u001b[0m Trial 153 finished with value: 0.6339513659477234 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 184, 'activation': 'relu', 'learning_rate': 0.010896751102315158, 'weight_decay': 0.0013854069368030783, 'l2_kernel': 0.014316556835036841, 'l2_bias': 0.4910755143783065, 'class_weight_0': 4.089889582366497, 'class_weight_1': 2.619860860674413}. Best is trial 41 with value: 0.6877726316452026.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:39:51,110]\u001b[0m Trial 154 finished with value: 0.624538242816925 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 175, 'activation': 'relu', 'learning_rate': 0.013207803839753327, 'weight_decay': 0.03016286542662465, 'l2_kernel': 0.03096090731351793, 'l2_bias': 0.47266493115709385, 'class_weight_0': 3.9704336505134834, 'class_weight_1': 3.1739427335260664}. Best is trial 41 with value: 0.6877726316452026.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:40:02,390]\u001b[0m Trial 155 finished with value: 0.6533754169940948 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 212, 'activation': 'relu', 'learning_rate': 0.01268115975128159, 'weight_decay': 0.01078312047570898, 'l2_kernel': 0.045017832336005224, 'l2_bias': 0.47999809905929036, 'class_weight_0': 3.996429762456288, 'class_weight_1': 2.431993467052934}. Best is trial 41 with value: 0.6877726316452026.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:40:14,646]\u001b[0m Trial 156 finished with value: 0.6712773442268372 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 190, 'activation': 'relu', 'learning_rate': 0.004786332311479154, 'weight_decay': 0.022104758392419675, 'l2_kernel': 0.024357837160248932, 'l2_bias': 0.49384011099445135, 'class_weight_0': 3.5939187828038404, 'class_weight_1': 2.5329984398348215}. Best is trial 41 with value: 0.6877726316452026.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:40:28,139]\u001b[0m Trial 157 finished with value: 0.7009804248809814 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 191, 'activation': 'relu', 'learning_rate': 0.004576602912220221, 'weight_decay': 0.020445088792036915, 'l2_kernel': 0.01601782903659603, 'l2_bias': 0.49610062311635067, 'class_weight_0': 3.509234822499622, 'class_weight_1': 2.802693801569547}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:40:40,406]\u001b[0m Trial 158 finished with value: 0.6821682751178741 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 202, 'activation': 'relu', 'learning_rate': 0.005580930371601921, 'weight_decay': 0.02219139906660155, 'l2_kernel': 0.009858295190259127, 'l2_bias': 0.49469172307374565, 'class_weight_0': 3.5263385713994273, 'class_weight_1': 2.4987958913621036}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:40:51,767]\u001b[0m Trial 159 finished with value: 0.6233589053153992 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 198, 'activation': 'relu', 'learning_rate': 0.004175213884439806, 'weight_decay': 0.052114492602483645, 'l2_kernel': 0.018023578297258903, 'l2_bias': 0.4960092251749409, 'class_weight_0': 3.51281038684947, 'class_weight_1': 2.565345190685798}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:41:02,756]\u001b[0m Trial 160 finished with value: 0.6013995707035065 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 209, 'activation': 'relu', 'learning_rate': 0.0008969241829903862, 'weight_decay': 0.0400491880650312, 'l2_kernel': 0.007013831803154107, 'l2_bias': 0.4723248972694574, 'class_weight_0': 3.405777374383873, 'class_weight_1': 3.0142393431618353}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:41:14,544]\u001b[0m Trial 161 finished with value: 0.6716539263725281 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 191, 'activation': 'relu', 'learning_rate': 0.006491019377286571, 'weight_decay': 0.02118512040512064, 'l2_kernel': 0.010183023394837586, 'l2_bias': 0.4938976600953385, 'class_weight_0': 3.6074502938682818, 'class_weight_1': 2.4567381994969617}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:41:26,390]\u001b[0m Trial 162 finished with value: 0.6478829085826874 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 194, 'activation': 'relu', 'learning_rate': 0.00655778410292824, 'weight_decay': 0.023893746352136595, 'l2_kernel': 0.012016279747890215, 'l2_bias': 0.4620205045674081, 'class_weight_0': 3.5116519495066876, 'class_weight_1': 2.4891627633851607}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:41:36,667]\u001b[0m Trial 163 finished with value: 0.6411764919757843 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 188, 'activation': 'relu', 'learning_rate': 0.006329259282988621, 'weight_decay': 0.009275826854365978, 'l2_kernel': 0.026571751317404777, 'l2_bias': 0.4807642599387707, 'class_weight_0': 3.6094042796958776, 'class_weight_1': 2.7323292178001215}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:41:47,031]\u001b[0m Trial 164 finished with value: 0.6637254953384399 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 203, 'activation': 'relu', 'learning_rate': 0.0014366940599294478, 'weight_decay': 0.03290275040688789, 'l2_kernel': 0.009607091801901148, 'l2_bias': 0.49819441436607215, 'class_weight_0': 3.3142600898309604, 'class_weight_1': 2.6328574369783646}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:41:59,250]\u001b[0m Trial 165 finished with value: 0.699737161397934 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 203, 'activation': 'relu', 'learning_rate': 0.003741685424145363, 'weight_decay': 0.0012266628652846806, 'l2_kernel': 0.010085595326774555, 'l2_bias': 0.47291745534593943, 'class_weight_0': 3.29294031970099, 'class_weight_1': 2.927160833702056}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:42:13,316]\u001b[0m Trial 166 finished with value: 0.671923816204071 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 178, 'activation': 'relu', 'learning_rate': 0.0030867422809601596, 'weight_decay': 0.0027285296532203788, 'l2_kernel': 0.0002128478770014406, 'l2_bias': 0.472625020105563, 'class_weight_0': 3.4258337810178263, 'class_weight_1': 2.977556644099145}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:42:22,948]\u001b[0m Trial 167 finished with value: 0.5 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 176, 'activation': 'softmax', 'learning_rate': 0.0026261224460630167, 'weight_decay': 0.00044156895447081416, 'l2_kernel': 0.011464399862329465, 'l2_bias': 0.4730949044286773, 'class_weight_0': 3.4137733281893805, 'class_weight_1': 2.9242703279122337}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:42:34,128]\u001b[0m Trial 168 finished with value: 0.6496163606643677 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 223, 'activation': 'relu', 'learning_rate': 0.003394533084450593, 'weight_decay': 0.010042104743576969, 'l2_kernel': 0.16342597859048824, 'l2_bias': 0.4796779654187904, 'class_weight_0': 3.4751259618656367, 'class_weight_1': 3.165863193953894}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-06 10:42:44,862]\u001b[0m Trial 169 finished with value: 0.6835464537143707 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 191, 'activation': 'relu', 'learning_rate': 0.0058370878455874945, 'weight_decay': 0.024154589821024287, 'l2_kernel': 0.002164225446048027, 'l2_bias': 0.4983499167377951, 'class_weight_0': 3.7297156308973793, 'class_weight_1': 3.018583390289908}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:42:57,935]\u001b[0m Trial 170 finished with value: 0.6347222328186035 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 202, 'activation': 'relu', 'learning_rate': 0.006794709972378983, 'weight_decay': 0.0007281237633305522, 'l2_kernel': 0.0015541500628536483, 'l2_bias': 0.4992512749504899, 'class_weight_0': 3.728321855023454, 'class_weight_1': 3.0722630078869937}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:43:10,506]\u001b[0m Trial 171 finished with value: 0.6787581741809845 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 190, 'activation': 'relu', 'learning_rate': 0.005299782039746769, 'weight_decay': 0.024421903383790865, 'l2_kernel': 0.011564670172219018, 'l2_bias': 0.499865784809539, 'class_weight_0': 3.587121098231181, 'class_weight_1': 2.908575345692749}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:43:22,057]\u001b[0m Trial 172 finished with value: 0.6169592440128326 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 190, 'activation': 'relu', 'learning_rate': 0.007701614729788314, 'weight_decay': 0.022502731325426163, 'l2_kernel': 0.00037395337988681623, 'l2_bias': 0.4665891988871328, 'class_weight_0': 3.5624665108758276, 'class_weight_1': 2.8971066658217985}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:43:34,371]\u001b[0m Trial 173 finished with value: 0.6648337841033936 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 203, 'activation': 'relu', 'learning_rate': 0.005087258860100461, 'weight_decay': 0.0430765765179466, 'l2_kernel': 0.013963391008039937, 'l2_bias': 0.49942520499542253, 'class_weight_0': 3.315889543270874, 'class_weight_1': 2.787137027318575}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:43:46,823]\u001b[0m Trial 174 finished with value: 0.6260656118392944 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 200, 'activation': 'relu', 'learning_rate': 0.001996704977623218, 'weight_decay': 0.014739818015213124, 'l2_kernel': 0.010837839990593921, 'l2_bias': 0.4818711169950482, 'class_weight_0': 3.4512048828450244, 'class_weight_1': 3.145271161084609}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:43:58,624]\u001b[0m Trial 175 finished with value: 0.6746541261672974 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 167, 'activation': 'relu', 'learning_rate': 0.009590928847902094, 'weight_decay': 0.007418856354824163, 'l2_kernel': 0.02385196712799936, 'l2_bias': 0.4741313705480289, 'class_weight_0': 3.192159836355268, 'class_weight_1': 3.359748237977109}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:44:09,535]\u001b[0m Trial 176 finished with value: 0.6435794830322266 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 170, 'activation': 'relu', 'learning_rate': 0.008599708500973555, 'weight_decay': 0.007773846559222839, 'l2_kernel': 0.028521238787106114, 'l2_bias': 0.47637219950836523, 'class_weight_0': 3.104368753605942, 'class_weight_1': 3.027610900163235}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:44:19,501]\u001b[0m Trial 177 finished with value: 0.5 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 161, 'activation': 'relu', 'learning_rate': 0.0003391594639736116, 'weight_decay': 0.05319838329694892, 'l2_kernel': 0.011872228197585027, 'l2_bias': 0.46693132258742165, 'class_weight_0': 3.159461772565929, 'class_weight_1': 3.337517998264941}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:44:28,985]\u001b[0m Trial 178 finished with value: 0.5 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 163, 'activation': 'sigmoid', 'learning_rate': 0.009361686400145236, 'weight_decay': 0.024874331350466457, 'l2_kernel': 0.027244402863721512, 'l2_bias': 0.48402068705910295, 'class_weight_0': 3.2248086631119675, 'class_weight_1': 3.555812765638066}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:44:40,931]\u001b[0m Trial 179 finished with value: 0.6467475891113281 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 192, 'activation': 'relu', 'learning_rate': 0.005953998978486061, 'weight_decay': 0.04459690834273957, 'l2_kernel': 0.0004835559169336749, 'l2_bias': 0.4579868523548468, 'class_weight_0': 3.6472649454357815, 'class_weight_1': 2.8812096730637973}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:44:52,269]\u001b[0m Trial 180 finished with value: 0.658056229352951 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 178, 'activation': 'relu', 'learning_rate': 0.0032976912126003294, 'weight_decay': 0.009542792650529732, 'l2_kernel': 0.018439916754351814, 'l2_bias': 0.44858528354169086, 'class_weight_0': 3.361125080822734, 'class_weight_1': 3.4251095804278178}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:45:03,034]\u001b[0m Trial 181 finished with value: 0.6250781416893005 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 206, 'activation': 'relu', 'learning_rate': 0.016510918293625453, 'weight_decay': 0.021406092096646435, 'l2_kernel': 0.03213591057147912, 'l2_bias': 0.4993273188971341, 'class_weight_0': 3.7299682990687275, 'class_weight_1': 2.673581406102489}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:45:14,465]\u001b[0m Trial 182 finished with value: 0.6411764919757843 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 198, 'activation': 'relu', 'learning_rate': 0.009193659704284207, 'weight_decay': 0.03524912677991484, 'l2_kernel': 0.011433855207273394, 'l2_bias': 0.48514086603192164, 'class_weight_0': 3.7982261353880697, 'class_weight_1': 2.244497345513527}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:45:26,059]\u001b[0m Trial 183 finished with value: 0.6144501566886902 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 171, 'activation': 'relu', 'learning_rate': 0.004278342889081033, 'weight_decay': 0.008146166525206, 'l2_kernel': 0.021604334041168537, 'l2_bias': 0.4718306404809848, 'class_weight_0': 3.564011595605667, 'class_weight_1': 2.8129346138080447}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:45:37,538]\u001b[0m Trial 184 finished with value: 0.6026712357997894 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 191, 'activation': 'relu', 'learning_rate': 0.0068717118839604595, 'weight_decay': 0.027205314145051836, 'l2_kernel': 0.04011196766424968, 'l2_bias': 0.34602263523469257, 'class_weight_0': 3.661854725643301, 'class_weight_1': 2.346385838349764}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:45:48,975]\u001b[0m Trial 185 finished with value: 0.6503268182277679 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 221, 'activation': 'relu', 'learning_rate': 0.010031346401521257, 'weight_decay': 0.03741798766428738, 'l2_kernel': 0.009005338272805865, 'l2_bias': 0.4883559500919675, 'class_weight_0': 4.153626566320071, 'class_weight_1': 3.7397598231157025}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:46:00,653]\u001b[0m Trial 186 finished with value: 0.6378476619720459 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 189, 'activation': 'relu', 'learning_rate': 0.0028642962233289306, 'weight_decay': 0.018370651029376562, 'l2_kernel': 0.025673862164041494, 'l2_bias': 0.4792505461275934, 'class_weight_0': 3.8395708837421085, 'class_weight_1': 2.7068634391519315}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:46:12,377]\u001b[0m Trial 187 finished with value: 0.6438547670841217 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 196, 'activation': 'relu', 'learning_rate': 0.0057052191013878106, 'weight_decay': 0.0027622503974428793, 'l2_kernel': 0.19801353861729706, 'l2_bias': 0.46513389611972816, 'class_weight_0': 3.459592156361827, 'class_weight_1': 2.5926441268055007}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-06 10:46:23,287]\u001b[0m Trial 188 finished with value: 0.6832281351089478 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 180, 'activation': 'relu', 'learning_rate': 0.008169614897660112, 'weight_decay': 0.015467096479912472, 'l2_kernel': 0.0018660283710596262, 'l2_bias': 0.4907811615863089, 'class_weight_0': 3.5553321768693196, 'class_weight_1': 2.9743254442047466}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:46:33,953]\u001b[0m Trial 189 finished with value: 0.6672842800617218 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 181, 'activation': 'relu', 'learning_rate': 0.006923711075494429, 'weight_decay': 0.047654135933400565, 'l2_kernel': 0.0015000673398591277, 'l2_bias': 0.48791546449196577, 'class_weight_0': 3.2713695681748622, 'class_weight_1': 3.2538818159543945}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:46:45,185]\u001b[0m Trial 190 finished with value: 0.6439018249511719 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 217, 'activation': 'relu', 'learning_rate': 0.00991569270412413, 'weight_decay': 0.06750196049528767, 'l2_kernel': 0.01619586486161198, 'l2_bias': 0.49952334255781033, 'class_weight_0': 3.5741967495947433, 'class_weight_1': 4.115435533142483}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:46:56,120]\u001b[0m Trial 191 finished with value: 0.6648337841033936 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 184, 'activation': 'relu', 'learning_rate': 0.0033266495652951125, 'weight_decay': 0.016307105879049648, 'l2_kernel': 0.008629569807025683, 'l2_bias': 0.4764509147446634, 'class_weight_0': 3.4103620375980723, 'class_weight_1': 2.9992990540911775}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:47:07,075]\u001b[0m Trial 192 finished with value: 0.6733775436878204 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 172, 'activation': 'relu', 'learning_rate': 0.008166740155622585, 'weight_decay': 0.02828778047087912, 'l2_kernel': 0.00042909073881867293, 'l2_bias': 0.49214046615807544, 'class_weight_0': 3.731605712439298, 'class_weight_1': 3.0955872911138225}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:47:18,264]\u001b[0m Trial 193 finished with value: 0.6346404850482941 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 172, 'activation': 'relu', 'learning_rate': 0.0073988846377033455, 'weight_decay': 0.02901834245874524, 'l2_kernel': 0.008724658428949927, 'l2_bias': 0.4882156662870365, 'class_weight_0': 3.748002122033898, 'class_weight_1': 2.894639442236176}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:47:29,987]\u001b[0m Trial 194 finished with value: 0.681045800447464 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 150, 'activation': 'relu', 'learning_rate': 0.015514209123612493, 'weight_decay': 0.0006855644377038936, 'l2_kernel': 0.0006687763503880742, 'l2_bias': 0.4906496106511754, 'class_weight_0': 3.641669744758575, 'class_weight_1': 3.0808766498497477}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:47:42,428]\u001b[0m Trial 195 finished with value: 0.660030335187912 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 154, 'activation': 'relu', 'learning_rate': 0.013388269123584486, 'weight_decay': 0.0005575820434296219, 'l2_kernel': 0.00018140811001636503, 'l2_bias': 0.4738110261684418, 'class_weight_0': 3.631109394313139, 'class_weight_1': 3.0098442894461077}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:47:53,073]\u001b[0m Trial 196 finished with value: 0.633851945400238 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 166, 'activation': 'relu', 'learning_rate': 0.015166409462015093, 'weight_decay': 0.010381663135602041, 'l2_kernel': 0.021231904758454243, 'l2_bias': 0.4923466953642496, 'class_weight_0': 3.4908268792293216, 'class_weight_1': 3.394286417446357}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:48:03,424]\u001b[0m Trial 197 finished with value: 0.5 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 177, 'activation': 'relu', 'learning_rate': 0.000198558577210412, 'weight_decay': 0.025672513882960723, 'l2_kernel': 0.0007383400130467864, 'l2_bias': 0.4811174281569187, 'class_weight_0': 2.977043616717075, 'class_weight_1': 2.9362956364957142}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:48:13,913]\u001b[0m Trial 198 finished with value: 0.6910326182842255 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 182, 'activation': 'relu', 'learning_rate': 0.017239071449792757, 'weight_decay': 0.013161198888723188, 'l2_kernel': 0.029493868066526256, 'l2_bias': 0.464326863951391, 'class_weight_0': 3.7312367589069275, 'class_weight_1': 3.111147296871925}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:48:24,636]\u001b[0m Trial 199 finished with value: 0.561260312795639 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 181, 'activation': 'relu', 'learning_rate': 0.01661046393319465, 'weight_decay': 0.013414732746963663, 'l2_kernel': 0.2660976965475665, 'l2_bias': 0.08907319799930827, 'class_weight_0': 3.7232473177829224, 'class_weight_1': 3.11790484824471}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:48:34,482]\u001b[0m Trial 200 finished with value: 0.6296959519386292 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 128, 'activation': 'relu', 'learning_rate': 0.0189965328820517, 'weight_decay': 3.64073793407253e-05, 'l2_kernel': 0.018377051384933132, 'l2_bias': 0.4996268667494196, 'class_weight_0': 3.532017607553647, 'class_weight_1': 3.1995664444608987}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:48:46,695]\u001b[0m Trial 201 finished with value: 0.6767973899841309 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 196, 'activation': 'relu', 'learning_rate': 0.010713373004374548, 'weight_decay': 0.01844006402075962, 'l2_kernel': 0.0338851704977476, 'l2_bias': 0.4661681184771249, 'class_weight_0': 3.846291074097646, 'class_weight_1': 3.088077383404551}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:48:57,885]\u001b[0m Trial 202 finished with value: 0.6628636717796326 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 197, 'activation': 'relu', 'learning_rate': 0.011274387265572661, 'weight_decay': 0.016565268678793838, 'l2_kernel': 0.03340493727904471, 'l2_bias': 0.4673824497294842, 'class_weight_0': 3.8525518614784025, 'class_weight_1': 3.2519912085394593}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:49:09,010]\u001b[0m Trial 203 finished with value: 0.6486714780330658 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 188, 'activation': 'relu', 'learning_rate': 0.00860344143409787, 'weight_decay': 0.023448385759765437, 'l2_kernel': 0.02831142133809368, 'l2_bias': 0.4802050098933731, 'class_weight_0': 3.6560818006030873, 'class_weight_1': 3.084520971800428}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:49:20,903]\u001b[0m Trial 204 finished with value: 0.6571895480155945 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 207, 'activation': 'relu', 'learning_rate': 0.004916333131782177, 'weight_decay': 0.036266451841777486, 'l2_kernel': 0.015181523334901328, 'l2_bias': 0.4905689513351819, 'class_weight_0': 3.8544761189279972, 'class_weight_1': 3.036112300940152}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:49:31,931]\u001b[0m Trial 205 finished with value: 0.6563654243946075 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 194, 'activation': 'relu', 'learning_rate': 0.01410345765346244, 'weight_decay': 0.008324635898628207, 'l2_kernel': 0.008087589345677675, 'l2_bias': 0.4724213354378879, 'class_weight_0': 3.3691874352395756, 'class_weight_1': 2.7992722376495704}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:49:41,597]\u001b[0m Trial 206 finished with value: 0.5 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 174, 'activation': 'softmax', 'learning_rate': 0.016878305928091936, 'weight_decay': 0.02129156853845373, 'l2_kernel': 0.039952624039112664, 'l2_bias': 0.4538370995953807, 'class_weight_0': 3.5751344563723917, 'class_weight_1': 3.108871196294248}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-06 10:49:54,516]\u001b[0m Trial 207 finished with value: 0.6929667592048645 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 182, 'activation': 'relu', 'learning_rate': 0.009987153300015414, 'weight_decay': 9.293409668638397e-05, 'l2_kernel': 0.021257257760529724, 'l2_bias': 0.49943904713103116, 'class_weight_0': 3.719589713143314, 'class_weight_1': 3.520024046656335}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:50:06,833]\u001b[0m Trial 208 finished with value: 0.6653594672679901 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 183, 'activation': 'relu', 'learning_rate': 0.01055576511361343, 'weight_decay': 0.008957035072695374, 'l2_kernel': 0.000186615093848784, 'l2_bias': 0.4816176488289812, 'class_weight_0': 3.748009828351408, 'class_weight_1': 3.3393933515044347}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:50:19,094]\u001b[0m Trial 209 finished with value: 0.6687580049037933 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 215, 'activation': 'relu', 'learning_rate': 0.008353355495274972, 'weight_decay': 0.010894591657083414, 'l2_kernel': 0.019107078067618237, 'l2_bias': 0.49956995089633427, 'class_weight_0': 3.8307983987541525, 'class_weight_1': 3.51463300457509}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:50:29,505]\u001b[0m Trial 210 finished with value: 0.6705882251262665 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 147, 'activation': 'relu', 'learning_rate': 0.012754987888688493, 'weight_decay': 0.00018969341849468802, 'l2_kernel': 0.0338617690880592, 'l2_bias': 0.4632022913087312, 'class_weight_0': 3.6899614447131657, 'class_weight_1': 3.6724425645625596}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:50:40,895]\u001b[0m Trial 211 finished with value: 0.6611111462116241 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 190, 'activation': 'relu', 'learning_rate': 0.006274849596609529, 'weight_decay': 0.02909432224304731, 'l2_kernel': 0.025107923527636397, 'l2_bias': 0.49089872819003566, 'class_weight_0': 3.5309035331349725, 'class_weight_1': 2.971726019370801}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:50:53,258]\u001b[0m Trial 212 finished with value: 0.6581699550151825 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 179, 'activation': 'relu', 'learning_rate': 0.004635825530480147, 'weight_decay': 0.01886277529982608, 'l2_kernel': 0.010106356701651565, 'l2_bias': 0.4897770019642115, 'class_weight_0': 3.617661712979617, 'class_weight_1': 3.4704493207622007}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:51:04,550]\u001b[0m Trial 213 finished with value: 0.6569338142871857 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 201, 'activation': 'relu', 'learning_rate': 0.009992783233401166, 'weight_decay': 0.0351190744297529, 'l2_kernel': 0.01829070015414774, 'l2_bias': 0.4748210456674404, 'class_weight_0': 3.883975080534727, 'class_weight_1': 3.6095789648064405}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:51:15,253]\u001b[0m Trial 214 finished with value: 0.6342710852622986 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 187, 'activation': 'relu', 'learning_rate': 0.002920263426741672, 'weight_decay': 0.015988975847169105, 'l2_kernel': 0.00019019230098239704, 'l2_bias': 0.4994466744642163, 'class_weight_0': 4.060931536297237, 'class_weight_1': 2.8478184634940806}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:51:25,898]\u001b[0m Trial 215 finished with value: 0.6013427078723907 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 258, 'activation': 'relu', 'learning_rate': 0.00807316734206047, 'weight_decay': 0.3549270547544951, 'l2_kernel': 0.027307140824739146, 'l2_bias': 0.4844713660458364, 'class_weight_0': 3.7336775260658963, 'class_weight_1': 3.253720778776705}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:51:38,543]\u001b[0m Trial 216 finished with value: 0.6748366355895996 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 156, 'activation': 'relu', 'learning_rate': 0.015027180311230525, 'weight_decay': 0.02673816542410407, 'l2_kernel': 0.011108917769753444, 'l2_bias': 0.49038839100820414, 'class_weight_0': 3.4574550361438905, 'class_weight_1': 2.7285580099812856}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:51:49,371]\u001b[0m Trial 217 finished with value: 0.6622904241085052 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 166, 'activation': 'relu', 'learning_rate': 0.015374598623671177, 'weight_decay': 0.009957146334289641, 'l2_kernel': 0.010513602342617908, 'l2_bias': 0.46923112172061343, 'class_weight_0': 3.353114230658388, 'class_weight_1': 2.72461575058304}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:52:00,673]\u001b[0m Trial 218 finished with value: 0.6591503322124481 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 174, 'activation': 'sigmoid', 'learning_rate': 0.012668590049196776, 'weight_decay': 0.0005253562262512619, 'l2_kernel': 0.01078482661054395, 'l2_bias': 0.4817707189017788, 'class_weight_0': 3.231718605527468, 'class_weight_1': 2.9163316576460008}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:52:12,010]\u001b[0m Trial 219 finished with value: 0.6451620161533356 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 155, 'activation': 'relu', 'learning_rate': 0.01935667308192203, 'weight_decay': 0.0368403261099671, 'l2_kernel': 0.015471736496249066, 'l2_bias': 0.4562048162448349, 'class_weight_0': 3.4662726931312937, 'class_weight_1': 3.1025584982062577}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:52:22,473]\u001b[0m Trial 220 finished with value: 0.656862735748291 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 158, 'activation': 'relu', 'learning_rate': 0.017988382965610405, 'weight_decay': 0.027580151936531808, 'l2_kernel': 0.04397381890207344, 'l2_bias': 0.4876376711940812, 'class_weight_0': 3.943398717111695, 'class_weight_1': 3.198290951042901}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:52:34,431]\u001b[0m Trial 221 finished with value: 0.6577650010585785 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 195, 'activation': 'relu', 'learning_rate': 0.010649013004288737, 'weight_decay': 0.020523270382973736, 'l2_kernel': 0.02435835758400247, 'l2_bias': 0.4922312447214471, 'class_weight_0': 3.6065112748511505, 'class_weight_1': 2.663101107617752}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:52:45,618]\u001b[0m Trial 222 finished with value: 0.6568627655506134 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 183, 'activation': 'relu', 'learning_rate': 0.005610925274982594, 'weight_decay': 0.025275726668991236, 'l2_kernel': 0.008524911437973296, 'l2_bias': 0.4759751266312355, 'class_weight_0': 3.442925691795251, 'class_weight_1': 2.572997551479875}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:52:56,233]\u001b[0m Trial 223 finished with value: 0.6787084341049194 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 191, 'activation': 'relu', 'learning_rate': 0.014889571959806717, 'weight_decay': 0.012457586862492875, 'l2_kernel': 0.03290559877787186, 'l2_bias': 0.498908740601759, 'class_weight_0': 3.7754166850626567, 'class_weight_1': 2.9847442046845534}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:53:07,331]\u001b[0m Trial 224 finished with value: 0.6441816091537476 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 142, 'activation': 'relu', 'learning_rate': 0.015164806567003013, 'weight_decay': 0.012251376177249553, 'l2_kernel': 0.018170359112762706, 'l2_bias': 0.4990801344746626, 'class_weight_0': 3.7518977071564414, 'class_weight_1': 2.991666332663139}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:53:19,413]\u001b[0m Trial 225 finished with value: 0.6715686321258545 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 205, 'activation': 'relu', 'learning_rate': 0.012704897362218595, 'weight_decay': 0.008809961386855013, 'l2_kernel': 0.03594993809582605, 'l2_bias': 0.49994398022871084, 'class_weight_0': 3.8519706422592903, 'class_weight_1': 2.8170651270605247}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-06 10:53:30,060]\u001b[0m Trial 226 finished with value: 0.6266979277133942 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 198, 'activation': 'relu', 'learning_rate': 0.014625222290178564, 'weight_decay': 0.03630462341388156, 'l2_kernel': 0.007982171998646384, 'l2_bias': 0.48259481629228634, 'class_weight_0': 4.011177232362251, 'class_weight_1': 3.052536619238545}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:53:41,293]\u001b[0m Trial 227 finished with value: 0.606535941362381 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 170, 'activation': 'relu', 'learning_rate': 0.008201811100530714, 'weight_decay': 0.017588930158422545, 'l2_kernel': 0.36806886887158546, 'l2_bias': 0.4673263529230254, 'class_weight_0': 3.6610401069946126, 'class_weight_1': 2.9489940290176397}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:53:53,107]\u001b[0m Trial 228 finished with value: 0.6506252288818359 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 180, 'activation': 'relu', 'learning_rate': 0.01738094541346239, 'weight_decay': 0.00010361282214766624, 'l2_kernel': 0.03210814865520551, 'l2_bias': 0.4892648487976129, 'class_weight_0': 3.775260205797931, 'class_weight_1': 3.130208388847431}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:54:04,274]\u001b[0m Trial 229 finished with value: 0.6419011354446411 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 192, 'activation': 'relu', 'learning_rate': 0.009766997142696675, 'weight_decay': 0.04136027274709132, 'l2_kernel': 0.0011215029999794568, 'l2_bias': 0.47826849705904995, 'class_weight_0': 3.4805391302408935, 'class_weight_1': 2.78621092632172}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:54:16,102]\u001b[0m Trial 230 finished with value: 0.6535947918891907 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 164, 'activation': 'relu', 'learning_rate': 0.011639387452785279, 'weight_decay': 0.029713913295075993, 'l2_kernel': 0.018985341652867527, 'l2_bias': 0.4886686873276978, 'class_weight_0': 3.315708299836052, 'class_weight_1': 2.2631661935599445}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:54:26,731]\u001b[0m Trial 231 finished with value: 0.6747726500034332 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 200, 'activation': 'relu', 'learning_rate': 0.012673066475916723, 'weight_decay': 0.009877638571675099, 'l2_kernel': 0.03703304442754262, 'l2_bias': 0.49989738552766266, 'class_weight_0': 3.9036348280582245, 'class_weight_1': 2.8330126029143434}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:54:37,640]\u001b[0m Trial 232 finished with value: 0.6808503866195679 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 200, 'activation': 'relu', 'learning_rate': 0.014552338226579791, 'weight_decay': 0.010409421793512985, 'l2_kernel': 0.044150801120528466, 'l2_bias': 0.499701047019734, 'class_weight_0': 4.127766501915142, 'class_weight_1': 3.9092441694160858}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:54:48,828]\u001b[0m Trial 233 finished with value: 0.625639408826828 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 206, 'activation': 'relu', 'learning_rate': 0.014069289904619581, 'weight_decay': 0.010119755047351294, 'l2_kernel': 0.04744641041479915, 'l2_bias': 0.49880968479321885, 'class_weight_0': 4.14621228674682, 'class_weight_1': 2.8929281897606516}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:55:00,045]\u001b[0m Trial 234 finished with value: 0.6293549239635468 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 197, 'activation': 'relu', 'learning_rate': 0.016841824663339354, 'weight_decay': 0.00021552476377382883, 'l2_kernel': 0.040086345809926094, 'l2_bias': 0.48346527128901534, 'class_weight_0': 4.232997089964571, 'class_weight_1': 3.8729140387336645}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:55:10,715]\u001b[0m Trial 235 finished with value: 0.6400256156921387 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 201, 'activation': 'relu', 'learning_rate': 0.01378354377133605, 'weight_decay': 0.01380875045367172, 'l2_kernel': 0.028860504958228048, 'l2_bias': 0.4996187037692733, 'class_weight_0': 3.912524150296553, 'class_weight_1': 4.393727375306542}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:55:21,464]\u001b[0m Trial 236 finished with value: 0.6218954026699066 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 186, 'activation': 'relu', 'learning_rate': 0.011404279338695358, 'weight_decay': 0.01054230843242925, 'l2_kernel': 0.04706369432584996, 'l2_bias': 0.4737795074980454, 'class_weight_0': 4.069926644313418, 'class_weight_1': 3.01058810395374}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:55:32,579]\u001b[0m Trial 237 finished with value: 0.6509351134300232 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 177, 'activation': 'relu', 'learning_rate': 0.019189450821038928, 'weight_decay': 0.02376349452118817, 'l2_kernel': 0.03204043599607502, 'l2_bias': 0.48617906209646616, 'class_weight_0': 3.9711198937183223, 'class_weight_1': 3.9099524282748335}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:55:44,796]\u001b[0m Trial 238 finished with value: 0.682935506105423 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 212, 'activation': 'relu', 'learning_rate': 0.015312165844522677, 'weight_decay': 0.0305164138910312, 'l2_kernel': 0.024969073231362056, 'l2_bias': 0.46339557126772124, 'class_weight_0': 3.831756214132752, 'class_weight_1': 2.7398176986515366}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:55:55,498]\u001b[0m Trial 239 finished with value: 0.6039215922355652 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 213, 'activation': 'relu', 'learning_rate': 0.015826967541313238, 'weight_decay': 0.031038450452738113, 'l2_kernel': 0.02132210153240018, 'l2_bias': 0.44711427761005723, 'class_weight_0': 0.3201117753601155, 'class_weight_1': 2.7226400951053544}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:56:06,101]\u001b[0m Trial 240 finished with value: 0.6512632369995117 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 210, 'activation': 'relu', 'learning_rate': 0.013716068103965594, 'weight_decay': 0.044477047645865644, 'l2_kernel': 0.03809106166424397, 'l2_bias': 0.48884753765482625, 'class_weight_0': 3.826310468700501, 'class_weight_1': 2.835705685235382}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:56:17,200]\u001b[0m Trial 241 finished with value: 0.6457871794700623 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 201, 'activation': 'relu', 'learning_rate': 0.011572150488667873, 'weight_decay': 0.018072165794002913, 'l2_kernel': 0.016294191907120996, 'l2_bias': 0.4649165954383333, 'class_weight_0': 3.9586471419731017, 'class_weight_1': 2.9381990229382704}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:56:28,146]\u001b[0m Trial 242 finished with value: 0.6486928164958954 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 194, 'activation': 'relu', 'learning_rate': 0.009442561671882713, 'weight_decay': 0.009699052353971864, 'l2_kernel': 0.008840940579059878, 'l2_bias': 0.321294091623365, 'class_weight_0': 3.775817410028048, 'class_weight_1': 2.7509854414309682}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:56:39,440]\u001b[0m Trial 243 finished with value: 0.6716538667678833 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 113, 'activation': 'relu', 'learning_rate': 0.017729768015733713, 'weight_decay': 0.02514111919003037, 'l2_kernel': 0.026417684787670263, 'l2_bias': 0.4775517135704617, 'class_weight_0': 4.110259671072687, 'class_weight_1': 3.080184014489417}. Best is trial 157 with value: 0.7009804248809814.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:56:50,433]\u001b[0m Trial 244 finished with value: 0.7147058844566345 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 224, 'activation': 'relu', 'learning_rate': 0.014405369664987436, 'weight_decay': 0.008844031348287134, 'l2_kernel': 0.0008442313215686649, 'l2_bias': 0.49960347582667186, 'class_weight_0': 3.6971304115389914, 'class_weight_1': 3.2478186786138186}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-06 10:57:01,367]\u001b[0m Trial 245 finished with value: 0.6362225711345673 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 217, 'activation': 'relu', 'learning_rate': 0.01617817252306975, 'weight_decay': 0.01708081643087322, 'l2_kernel': 0.019366794562048397, 'l2_bias': 0.49962117702311604, 'class_weight_0': 3.6940511803461447, 'class_weight_1': 3.3927505781419685}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:57:12,598]\u001b[0m Trial 246 finished with value: 0.673437088727951 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 241, 'activation': 'relu', 'learning_rate': 0.014055647492035264, 'weight_decay': 0.03263578886311959, 'l2_kernel': 0.03265866576591006, 'l2_bias': 0.4919666366294139, 'class_weight_0': 3.8531830632976645, 'class_weight_1': 4.111123358943887}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:57:23,325]\u001b[0m Trial 247 finished with value: 0.6554472148418427 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 242, 'activation': 'relu', 'learning_rate': 0.014109926199733653, 'weight_decay': 0.0330369806649177, 'l2_kernel': 0.028310368261466917, 'l2_bias': 0.49056153291606763, 'class_weight_0': 3.827421575948122, 'class_weight_1': 4.151064409748702}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:57:34,794]\u001b[0m Trial 248 finished with value: 0.6227337419986725 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 231, 'activation': 'relu', 'learning_rate': 0.011562289314860263, 'weight_decay': 0.04874378370131082, 'l2_kernel': 0.0003413405041804828, 'l2_bias': 0.48830866438823894, 'class_weight_0': 3.9993010046435558, 'class_weight_1': 4.131832237496778}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:57:43,989]\u001b[0m Trial 249 finished with value: 0.5 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 230, 'activation': 'softmax', 'learning_rate': 0.015459554880389017, 'weight_decay': 0.03833016719772583, 'l2_kernel': 0.33659094850102295, 'l2_bias': 0.49057675855541594, 'class_weight_0': 3.904519697994232, 'class_weight_1': 3.2977523899388324}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:57:55,966]\u001b[0m Trial 250 finished with value: 0.6705882549285889 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 256, 'activation': 'relu', 'learning_rate': 0.013042021868639621, 'weight_decay': 0.008347694955622726, 'l2_kernel': 0.013954837424722918, 'l2_bias': 0.4991938278440785, 'class_weight_0': 3.712980056706185, 'class_weight_1': 3.9786315509059107}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:58:06,072]\u001b[0m Trial 251 finished with value: 0.6679738759994507 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 236, 'activation': 'relu', 'learning_rate': 0.020023088905221687, 'weight_decay': 0.05705363715298639, 'l2_kernel': 0.03573939407334036, 'l2_bias': 0.4801171616757024, 'class_weight_0': 3.8979034372614016, 'class_weight_1': 4.332018635089046}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:58:19,047]\u001b[0m Trial 252 finished with value: 0.6290139555931091 and parameters: {'n_hidden_layers': 2, 'n_hidden_nodes': 249, 'activation': 'relu', 'learning_rate': 0.017573445198259582, 'weight_decay': 0.02485417072327921, 'l2_kernel': 0.017211401298457767, 'l2_bias': 0.4825425862901316, 'class_weight_0': 4.1932885247196, 'class_weight_1': 3.2024576605360076}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:58:29,767]\u001b[0m Trial 253 finished with value: 0.6623188257217407 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 226, 'activation': 'relu', 'learning_rate': 0.010005576073934763, 'weight_decay': 0.01646042351606221, 'l2_kernel': 0.009168536771830976, 'l2_bias': 0.499820805440933, 'class_weight_0': 4.047452974084639, 'class_weight_1': 3.296900469496279}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:58:39,496]\u001b[0m Trial 254 finished with value: 0.5 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 208, 'activation': 'sigmoid', 'learning_rate': 0.007839468999154435, 'weight_decay': 0.02915852465665719, 'l2_kernel': 0.02426791720485448, 'l2_bias': 0.49028276549810307, 'class_weight_0': 3.7779122414756565, 'class_weight_1': 3.99637757317845}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:58:50,526]\u001b[0m Trial 255 finished with value: 0.6633986830711365 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 203, 'activation': 'relu', 'learning_rate': 0.012869504656658547, 'weight_decay': 0.1592911969531544, 'l2_kernel': 0.00013523414288720004, 'l2_bias': 0.4731413710021555, 'class_weight_0': 3.6549828163625895, 'class_weight_1': 2.6329701141180695}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:59:01,742]\u001b[0m Trial 256 finished with value: 0.658128172159195 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 187, 'activation': 'relu', 'learning_rate': 0.015316932551116677, 'weight_decay': 0.00782671141709775, 'l2_kernel': 0.014549759334632444, 'l2_bias': 0.4820073296273829, 'class_weight_0': 3.890130505717856, 'class_weight_1': 3.16534793678367}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:59:12,914]\u001b[0m Trial 257 finished with value: 0.6509804129600525 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 195, 'activation': 'relu', 'learning_rate': 0.009966430226757897, 'weight_decay': 0.01787285131693008, 'l2_kernel': 0.04173818389770234, 'l2_bias': 0.06598138024119832, 'class_weight_0': 4.068966021286523, 'class_weight_1': 3.070599373100852}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:59:23,017]\u001b[0m Trial 258 finished with value: 0.6441886723041534 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 199, 'activation': 'relu', 'learning_rate': 0.018574890407883536, 'weight_decay': 0.037523455761263605, 'l2_kernel': 0.027126709370947834, 'l2_bias': 0.46145256588283534, 'class_weight_0': 3.549679396369721, 'class_weight_1': 2.8806180142946918}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:59:35,870]\u001b[0m Trial 259 finished with value: 0.6564293801784515 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 189, 'activation': 'relu', 'learning_rate': 0.012137332578453622, 'weight_decay': 0.0013498143043820306, 'l2_kernel': 0.14614260398027784, 'l2_bias': 0.1594341160385518, 'class_weight_0': 3.7727889024980046, 'class_weight_1': 4.570945174457945}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:59:47,691]\u001b[0m Trial 260 finished with value: 0.6254476010799408 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 261, 'activation': 'relu', 'learning_rate': 0.006880229282652778, 'weight_decay': 0.028360051609747083, 'l2_kernel': 0.398801851953125, 'l2_bias': 0.49976398490683954, 'class_weight_0': 3.6693977505773994, 'class_weight_1': 2.6669553072346335}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 10:59:57,762]\u001b[0m Trial 261 finished with value: 0.6309036612510681 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 212, 'activation': 'relu', 'learning_rate': 0.015539494376621664, 'weight_decay': 0.015452237308797346, 'l2_kernel': 0.00881996272801248, 'l2_bias': 0.49102150717226345, 'class_weight_0': 3.96553979355374, 'class_weight_1': 2.8552755289480385}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:00:09,417]\u001b[0m Trial 262 finished with value: 0.6699244379997253 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 185, 'activation': 'relu', 'learning_rate': 0.008376176001057598, 'weight_decay': 0.041156007393287265, 'l2_kernel': 0.021384365279277038, 'l2_bias': 0.4792503938816579, 'class_weight_0': 4.255163928093788, 'class_weight_1': 3.1951851374903897}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:00:20,829]\u001b[0m Trial 263 finished with value: 0.6146974265575409 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 194, 'activation': 'relu', 'learning_rate': 0.013661009371944649, 'weight_decay': 0.3695897387980613, 'l2_kernel': 0.03420871245635448, 'l2_bias': 0.4715722195166917, 'class_weight_0': 3.849386222338239, 'class_weight_1': 4.216413718611107}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-06 11:00:32,666]\u001b[0m Trial 264 finished with value: 0.6525007486343384 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 205, 'activation': 'relu', 'learning_rate': 0.011208836418435012, 'weight_decay': 0.31402016733861804, 'l2_kernel': 0.009217036703823766, 'l2_bias': 0.4901296119309874, 'class_weight_0': 3.580911553645616, 'class_weight_1': 2.3475565602240316}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:00:43,568]\u001b[0m Trial 265 finished with value: 0.6671994924545288 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 220, 'activation': 'relu', 'learning_rate': 0.016443908353681376, 'weight_decay': 0.0005612803096973693, 'l2_kernel': 0.018434959344957143, 'l2_bias': 0.49981169684423316, 'class_weight_0': 3.7186978924791054, 'class_weight_1': 2.9931436650782994}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:00:55,597]\u001b[0m Trial 266 finished with value: 0.6685222089290619 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 171, 'activation': 'relu', 'learning_rate': 0.021876097817217326, 'weight_decay': 0.021814726821974374, 'l2_kernel': 0.04479932311289454, 'l2_bias': 0.014667972034933613, 'class_weight_0': 3.802028821695599, 'class_weight_1': 3.349046613126431}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:01:10,048]\u001b[0m Trial 267 finished with value: 0.6549019515514374 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 162, 'activation': 'relu', 'learning_rate': 0.005858488372389764, 'weight_decay': 0.010754270074088721, 'l2_kernel': 0.00033027544344103784, 'l2_bias': 0.48251467431079054, 'class_weight_0': 4.145462312746713, 'class_weight_1': 2.1896631097570034}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:01:21,213]\u001b[0m Trial 268 finished with value: 0.6297385692596436 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 182, 'activation': 'relu', 'learning_rate': 0.014068292043177229, 'weight_decay': 0.03089698571676648, 'l2_kernel': 0.03165046355929045, 'l2_bias': 0.46741857729534125, 'class_weight_0': 2.6150432417064295, 'class_weight_1': 2.759267316292685}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:01:30,853]\u001b[0m Trial 269 finished with value: 0.5 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 201, 'activation': 'softmax', 'learning_rate': 0.0673203601681305, 'weight_decay': 0.05488644108256974, 'l2_kernel': 0.010820714264301446, 'l2_bias': 0.49031298772539395, 'class_weight_0': 3.516079160234138, 'class_weight_1': 3.1241157166144076}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:01:41,787]\u001b[0m Trial 270 finished with value: 0.6445012986660004 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 246, 'activation': 'relu', 'learning_rate': 0.07080105406933306, 'weight_decay': 0.020117962514017894, 'l2_kernel': 0.02342406055905267, 'l2_bias': 0.4578391562756797, 'class_weight_0': 3.9262131433276286, 'class_weight_1': 3.4934529884157066}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:01:52,232]\u001b[0m Trial 271 finished with value: 0.655254065990448 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 192, 'activation': 'relu', 'learning_rate': 0.008720260690776964, 'weight_decay': 0.010863324487742621, 'l2_kernel': 0.015451564807865189, 'l2_bias': 0.4784823394658899, 'class_weight_0': 1.347411619602548, 'class_weight_1': 2.610795398836693}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:02:04,120]\u001b[0m Trial 272 finished with value: 0.6658661365509033 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 276, 'activation': 'relu', 'learning_rate': 0.01067115745946908, 'weight_decay': 0.03011019102647813, 'l2_kernel': 0.04966304922290948, 'l2_bias': 0.48909502369891933, 'class_weight_0': 3.638644108415569, 'class_weight_1': 2.973788257094958}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:02:15,666]\u001b[0m Trial 273 finished with value: 0.6277920007705688 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 265, 'activation': 'relu', 'learning_rate': 0.004191132703397966, 'weight_decay': 0.043478927210443136, 'l2_kernel': 0.12378789361557382, 'l2_bias': 0.47206391517270113, 'class_weight_0': 2.0809546864833135, 'class_weight_1': 2.856105776381461}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:02:26,324]\u001b[0m Trial 274 finished with value: 0.6839869320392609 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 175, 'activation': 'relu', 'learning_rate': 0.019430312400702823, 'weight_decay': 0.01901063701786806, 'l2_kernel': 0.033330071081233155, 'l2_bias': 0.49142212849531586, 'class_weight_0': 4.073241888265479, 'class_weight_1': 3.0426231084323256}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:02:37,896]\u001b[0m Trial 275 finished with value: 0.6486786007881165 and parameters: {'n_hidden_layers': 2, 'n_hidden_nodes': 182, 'activation': 'relu', 'learning_rate': 0.021213120664753983, 'weight_decay': 0.008081196528718096, 'l2_kernel': 0.03654876415313445, 'l2_bias': 0.1867768177979042, 'class_weight_0': 4.0755166258474675, 'class_weight_1': 2.7697421097558874}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:02:48,486]\u001b[0m Trial 276 finished with value: 0.5773235261440277 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 188, 'activation': 'sigmoid', 'learning_rate': 0.019455183209293368, 'weight_decay': 0.0009185016068167552, 'l2_kernel': 0.028330427358177396, 'l2_bias': 0.49980044296030063, 'class_weight_0': 4.3122955962713645, 'class_weight_1': 3.239472259132925}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:03:00,032]\u001b[0m Trial 277 finished with value: 0.5 and parameters: {'n_hidden_layers': 3, 'n_hidden_nodes': 208, 'activation': 'relu', 'learning_rate': 0.017507607311079598, 'weight_decay': 0.016900597329412754, 'l2_kernel': 0.05512079217031142, 'l2_bias': 0.4835833800574203, 'class_weight_0': 0.751211732458064, 'class_weight_1': 2.927649357191249}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:03:10,300]\u001b[0m Trial 278 finished with value: 0.6320261657238007 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 199, 'activation': 'relu', 'learning_rate': 0.015315142331082549, 'weight_decay': 0.02115075702220789, 'l2_kernel': 0.04131457834695199, 'l2_bias': 0.46333861057123804, 'class_weight_0': 4.032151744898665, 'class_weight_1': 2.4860443365866787}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:03:21,480]\u001b[0m Trial 279 finished with value: 0.6545751690864563 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 240, 'activation': 'relu', 'learning_rate': 0.017585334309576723, 'weight_decay': 0.010043715763694106, 'l2_kernel': 0.027830979411264334, 'l2_bias': 0.4796379515251609, 'class_weight_0': 4.179373786705228, 'class_weight_1': 2.6860292663836685}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:03:32,663]\u001b[0m Trial 280 finished with value: 0.6806266009807587 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 179, 'activation': 'relu', 'learning_rate': 0.013372047942120524, 'weight_decay': 0.03634828697245129, 'l2_kernel': 0.019805819889941716, 'l2_bias': 0.49008903881053956, 'class_weight_0': 3.991289476078769, 'class_weight_1': 3.037838644272397}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:03:43,683]\u001b[0m Trial 281 finished with value: 0.6578431725502014 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 177, 'activation': 'relu', 'learning_rate': 0.01946343911628418, 'weight_decay': 0.020019171069408682, 'l2_kernel': 0.01754607460841466, 'l2_bias': 0.4733137927942671, 'class_weight_0': 4.003584005641693, 'class_weight_1': 3.02602759942899}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:03:55,506]\u001b[0m Trial 282 finished with value: 0.6352941691875458 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 147, 'activation': 'relu', 'learning_rate': 0.011494379101946586, 'weight_decay': 0.05064549898389186, 'l2_kernel': 0.0218294958410481, 'l2_bias': 0.49992337343474247, 'class_weight_0': 4.109738221258981, 'class_weight_1': 3.077688834219503}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-06 11:04:04,438]\u001b[0m Trial 283 finished with value: 0.5 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 181, 'activation': 'relu', 'learning_rate': 0.002396199661334114, 'weight_decay': 0.3411623317569552, 'l2_kernel': 0.21767398167062485, 'l2_bias': 0.44648528668185034, 'class_weight_0': 2.4090916789677577, 'class_weight_1': 2.9362187629126235}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:04:16,472]\u001b[0m Trial 284 finished with value: 0.6340437531471252 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 175, 'activation': 'relu', 'learning_rate': 0.0125787655335805, 'weight_decay': 0.0001917602639008205, 'l2_kernel': 0.01157940090876966, 'l2_bias': 0.48912577050865863, 'class_weight_0': 3.9501922917701906, 'class_weight_1': 2.8312053008890965}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:04:26,404]\u001b[0m Trial 285 finished with value: 0.6480392515659332 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 191, 'activation': 'relu', 'learning_rate': 0.0160121499768964, 'weight_decay': 0.01076267641286143, 'l2_kernel': 0.009007188853096777, 'l2_bias': 0.48163330329502196, 'class_weight_0': 3.4009610250587023, 'class_weight_1': 3.2550598290257504}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:04:38,589]\u001b[0m Trial 286 finished with value: 0.6602230668067932 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 196, 'activation': 'relu', 'learning_rate': 0.006773385608763708, 'weight_decay': 0.03842710255571476, 'l2_kernel': 0.022421955567358355, 'l2_bias': 0.4642609288177823, 'class_weight_0': 4.259319868271311, 'class_weight_1': 3.157597870358649}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:04:49,877]\u001b[0m Trial 287 finished with value: 0.6206592619419098 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 170, 'activation': 'relu', 'learning_rate': 0.009940146717703994, 'weight_decay': 0.025985300902019448, 'l2_kernel': 0.04012460490927084, 'l2_bias': 0.4904297299420732, 'class_weight_0': 3.530105821566442, 'class_weight_1': 2.3787291801843304}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:05:00,966]\u001b[0m Trial 288 finished with value: 0.6660131216049194 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 186, 'activation': 'relu', 'learning_rate': 0.013256554580617983, 'weight_decay': 0.015695688250339044, 'l2_kernel': 0.010423185689971583, 'l2_bias': 0.4549683983186395, 'class_weight_0': 3.8885125758790147, 'class_weight_1': 3.648057043903913}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:05:12,422]\u001b[0m Trial 289 finished with value: 0.6510949730873108 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 300, 'activation': 'relu', 'learning_rate': 0.019069684105721494, 'weight_decay': 0.03522662342079978, 'l2_kernel': 0.029261544717332895, 'l2_bias': 0.10754945977053618, 'class_weight_0': 2.2277767840794978, 'class_weight_1': 3.0328549571167462}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:05:25,614]\u001b[0m Trial 290 finished with value: 0.6849673092365265 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 160, 'activation': 'relu', 'learning_rate': 0.021223277971588406, 'weight_decay': 0.022280788150453944, 'l2_kernel': 0.0005976747625609407, 'l2_bias': 0.4745545339884581, 'class_weight_0': 4.014385434379597, 'class_weight_1': 2.5822483782011156}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:05:36,837]\u001b[0m Trial 291 finished with value: 0.6775078475475311 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 194, 'activation': 'relu', 'learning_rate': 0.019628812548578373, 'weight_decay': 0.04830474709731069, 'l2_kernel': 0.00022630537807939981, 'l2_bias': 0.48221655874629743, 'class_weight_0': 4.137164529605782, 'class_weight_1': 2.578008175970133}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:05:47,478]\u001b[0m Trial 292 finished with value: 0.6376713514328003 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 154, 'activation': 'relu', 'learning_rate': 0.021889024572486936, 'weight_decay': 0.06008421776997486, 'l2_kernel': 0.00045152728275536097, 'l2_bias': 0.4697432873535303, 'class_weight_0': 4.140462323930261, 'class_weight_1': 2.605002347822567}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:05:57,496]\u001b[0m Trial 293 finished with value: 0.5 and parameters: {'n_hidden_layers': 2, 'n_hidden_nodes': 191, 'activation': 'softmax', 'learning_rate': 0.021321800426849443, 'weight_decay': 0.07422539857214061, 'l2_kernel': 0.0006226092657536549, 'l2_bias': 0.48089773368837574, 'class_weight_0': 4.067099976981923, 'class_weight_1': 2.556653970849336}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:06:08,385]\u001b[0m Trial 294 finished with value: 0.6359477043151855 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 138, 'activation': 'relu', 'learning_rate': 0.02288055387031028, 'weight_decay': 0.04881837087304299, 'l2_kernel': 4.507276587858641e-05, 'l2_bias': 0.48333624085865035, 'class_weight_0': 4.203514963036026, 'class_weight_1': 2.4224227936339213}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:06:18,513]\u001b[0m Trial 295 finished with value: 0.6496732234954834 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 184, 'activation': 'relu', 'learning_rate': 0.019962050578076363, 'weight_decay': 0.0392352753147435, 'l2_kernel': 0.01219433936037304, 'l2_bias': 0.4598386887939286, 'class_weight_0': 4.0490590095259815, 'class_weight_1': 2.519479027210914}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:06:29,244]\u001b[0m Trial 296 finished with value: 0.6316993534564972 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 216, 'activation': 'relu', 'learning_rate': 0.017583963773751772, 'weight_decay': 0.050434676368852085, 'l2_kernel': 0.011141274768192362, 'l2_bias': 0.4745295562150441, 'class_weight_0': 4.3296177175203905, 'class_weight_1': 2.6824126129077657}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:06:40,960]\u001b[0m Trial 297 finished with value: 0.6655730605125427 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 158, 'activation': 'relu', 'learning_rate': 0.020364345237252723, 'weight_decay': 0.06772125129782755, 'l2_kernel': 0.009178219344335874, 'l2_bias': 0.4903855781705428, 'class_weight_0': 4.0082610469817705, 'class_weight_1': 2.3073019716564653}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:06:52,338]\u001b[0m Trial 298 finished with value: 0.6526143550872803 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 195, 'activation': 'relu', 'learning_rate': 0.01700521923516388, 'weight_decay': 0.028896934947824894, 'l2_kernel': 0.018528150611797287, 'l2_bias': 0.49055083326258075, 'class_weight_0': 3.797558769129037, 'class_weight_1': 2.7171834484717943}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n",
      "\u001b[32m[I 2022-07-06 11:07:02,429]\u001b[0m Trial 299 finished with value: 0.5 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 205, 'activation': 'sigmoid', 'learning_rate': 0.0188296821015123, 'weight_decay': 0.04240551564769666, 'l2_kernel': 0.0078121443409584265, 'l2_bias': 0.13769170456898794, 'class_weight_0': 4.179265606793209, 'class_weight_1': 2.587432781430685}. Best is trial 244 with value: 0.7147058844566345.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study_is_sig3.optimize(compute_mlp_performance, n_trials=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d95dafc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=244, values=[0.7147058844566345], datetime_start=datetime.datetime(2022, 7, 6, 10, 56, 39, 441230), datetime_complete=datetime.datetime(2022, 7, 6, 10, 56, 50, 432565), params={'n_hidden_layers': 1, 'n_hidden_nodes': 224, 'activation': 'relu', 'learning_rate': 0.014405369664987436, 'weight_decay': 0.008844031348287134, 'l2_kernel': 0.0008442313215686649, 'l2_bias': 0.49960347582667186, 'class_weight_0': 3.6971304115389914, 'class_weight_1': 3.2478186786138186}, distributions={'n_hidden_layers': IntUniformDistribution(high=3, low=1, step=1), 'n_hidden_nodes': IntUniformDistribution(high=300, low=20, step=1), 'activation': CategoricalDistribution(choices=('relu', 'sigmoid', 'softmax')), 'learning_rate': UniformDistribution(high=0.1, low=1e-09), 'weight_decay': UniformDistribution(high=0.5, low=0.0), 'l2_kernel': UniformDistribution(high=0.5, low=0.0), 'l2_bias': UniformDistribution(high=0.5, low=0.0), 'class_weight_0': UniformDistribution(high=5.0, low=0.0), 'class_weight_1': UniformDistribution(high=5.0, low=0.0)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=244, state=TrialState.COMPLETE, value=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_is_sig3.best_trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
