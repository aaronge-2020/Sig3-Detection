{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b4cbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import median_absolute_deviation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import tensorflow_addons as tfa\n",
    "random_state = 1234\n",
    "from keras.layers import Dropout\n",
    "from numpy.random import seed\n",
    "seed(random_state)\n",
    "tf.random.set_seed(random_state)\n",
    "from keras.models import load_model\n",
    "import copy\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f9f3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def79dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196c46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c73992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"D:\\NIH\\Mutational-Spectrum-master\\Data\\New\\MSK_Impact_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e35b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_dir(dirname):\n",
    "    dir_files = list()\n",
    "    for root, _, files in os.walk(dirname):\n",
    "        for file in files:\n",
    "            dir_files.append(os.path.join(root, file))\n",
    "    \n",
    "    return dir_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a981e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With overfitting\n",
    "def build_mlp_model(input_shape=(96,), n_hidden_layers=1, n_hidden_nodes=8, activation='relu',learning_rate=0.001, weight_decay = 0, dropout=0.2):\n",
    "    # optimizer parameters\n",
    "    loss = \"binary_crossentropy\"\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay = weight_decay)\n",
    "    metrics = keras.metrics.AUC(name='auc')\n",
    "    \n",
    "    model = load_model(r\"D:\\NIH\\Mutational-Spectrum-master\\Scripts\\Autoencoder\\MSKIMPACT_encoder.h5\")\n",
    "    for layer in model.layers:\n",
    "          layer.trainable = False\n",
    "    \n",
    "    for i in range(n_hidden_layers):\n",
    "        model.add(keras.layers.Dense(n_hidden_nodes, activation=activation, name = 'dense_layer_' + str(i)))\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"prediction_layer\"))\n",
    "    \n",
    "    # optimizer\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04df7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without overfitting\n",
    "def build_mlp_model( input_shape=(96,), learning_rate=0.001, weight_decay = 0, dropout=0.2):\n",
    "    # optimizer parameters\n",
    "    loss = \"binary_crossentropy\"\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay = weight_decay)\n",
    "    metrics = keras.metrics.AUC(name='auc')\n",
    "    \n",
    "    model = load_model(r\"D:\\NIH\\Mutational-Spectrum-master\\Scripts\\Autoencoder\\MSKIMPACT_encoder.h5\")\n",
    "    for layer in model.layers:\n",
    "          layer.trainable = False\n",
    "    \n",
    "#     for i in range(n_hidden_layers):\n",
    "#         model.add(keras.layers.Dense(n_hidden_nodes, activation=activation, name = 'dense_layer_' + str(i)))\n",
    "#     model.add(Dropout(dropout))\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"prediction_layer\"))\n",
    "    \n",
    "    # optimizer\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d22a3f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_row_indices_with_sum_zero(X):\n",
    "    return X.index[(X.sum(axis=1) == 0)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b835051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization\n",
    "def scale_row(X):\n",
    "    for column in X.columns:\n",
    "        X[column] = (X[column] -\n",
    "                               X[column].mean()) / X[column].std() \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a80587c5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dataset_generator(data_dir, num_files, y_col):\n",
    "\n",
    "    # Would take eons to run if I didn't limit this to only the first file  \n",
    "    for datafile in random.sample(list_files_in_dir(data_dir), num_files):\n",
    "        data = pd.read_csv(datafile)\n",
    "        X = data.iloc[:, :96]\n",
    "        y = (data[y_col]).astype(np.int_)\n",
    "#         X.columns = X.columns.str.replace('[', '').str.replace(']', '').str.replace('>', '')\n",
    "        X = scale_row(X)\n",
    "    yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b05e467",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def scale_data(df):\n",
    "    # Define the scaler \n",
    "    scaler = StandardScaler().fit(df)\n",
    "    # Scales each individual row   \n",
    "    df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "893fb89e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_val_test_generator(data_dir, num_samples, num_folds, y_col, test_frac=0.1):\n",
    "    val_frac = test_frac/(1.0 - test_frac)\n",
    "    \n",
    "    for X, y in dataset_generator(data_dir, num_samples, y_col):\n",
    "        fold_data = list()\n",
    "        skf = StratifiedKFold(n_splits=num_folds, shuffle = True, random_state = random_state)\n",
    "#         skf = StratifiedKFold(n_splits=num_folds)\n",
    "        \n",
    "        for train_val_index, test_index in skf.split(X, y):\n",
    "            X_test, y_test = X.iloc[test_index, :], y.iloc[test_index]\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X.iloc[train_val_index,:], y.iloc[train_val_index], \n",
    "                test_size=val_frac, \n",
    "                random_state=random_state, \n",
    "                stratify=y.iloc[train_val_index]\n",
    "            )\n",
    "            #X_train, X_val, X_test = scale_columns(X_train, X_val, X_test)\n",
    "            fold_data.append(((X_train, y_train), \n",
    "                              (X_val, y_val), \n",
    "                              (X_test, y_test)))\n",
    "        yield fold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7300da92",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compute_mlp_performance(trial, data_dir=data_dir, n_folds=10, input_shape=(96,), \n",
    "                            n_hidden_layers=2, n_hidden_nodes=16, \n",
    "                            activation=\"relu\", learning_rate=0.001, num_samples = 1, y_col = 'is_sig3_20', weight_decay = 0, dropout=0.2):\n",
    "    aucs = list()\n",
    "    models = list()\n",
    "    \n",
    "    # 60-20-20 split\n",
    "    test_frac=1.0/float(n_folds)\n",
    "#     fpr_list = []\n",
    "#     tpr_list = []\n",
    "#     roc_auc_list = []\n",
    "    \n",
    "    for folds_data in train_val_test_generator(data_dir, num_samples=num_samples, num_folds=n_folds, y_col=y_col):\n",
    "        fold_aucs = list()\n",
    "        fold_models = list()\n",
    "        \n",
    "        for fold_data in folds_data:\n",
    "            # get data\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test) = fold_data\n",
    "            # build model and ensure that parameters passed in are within the normal range\n",
    "            # if we don't type cast as integers, bayesian optimizer will guess float values\n",
    "            model = build_mlp_model(input_shape, \n",
    "                                    trial.suggest_int('n_hidden_layers', 0, 8), \n",
    "                                    trial.suggest_int('n_hidden_nodes', 1, 128), \n",
    "                                    trial.suggest_categorical(\"activation\", [\"relu\", \"sigmoid\", \"softmax\"]), \n",
    "                                    trial.suggest_float('learning_rate', 1e-6, 1e-1), \n",
    "                                    trial.suggest_float('weight_decay', 0, 1e-1))\n",
    "#             model = build_mlp_model(input_shape, \n",
    "#                         trial.suggest_float('learning_rate', 1e-6, 1e-1), \n",
    "#                         trial.suggest_float('weight_decay', 0, 1e-1),\n",
    "#                         trial.suggest_float('dropout', 0, 5e-1))\n",
    "\n",
    "            model.fit(X_train, y_train, \n",
    "                      validation_data=(X_val, y_val), \n",
    "                      epochs=1000, batch_size=32, verbose=0,\n",
    "                      callbacks=[keras.callbacks.EarlyStopping(monitor='val_auc', patience=5)])\n",
    "            \n",
    "            # evaluate\n",
    "            y_score = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "            fold_aucs.append(y_score)\n",
    "            fold_models.append(model)\n",
    "        aucs.append(fold_aucs)\n",
    "        models.append(fold_models)\n",
    "        \n",
    "    # Gets median index value for all the different samples (rows)  \n",
    "    medianIndices = [indices[len(aucs[0])//2] for indices in np.argsort(aucs, axis=1)]\n",
    "    medianValues = [values[index] for values, index in zip(aucs, medianIndices)]\n",
    "    \n",
    "    # Gets the file which contains the median of median value\n",
    "    fileInd = np.argsort(medianValues)[len(medianValues)//2]\n",
    "    \n",
    "    aucs = np.array(aucs)\n",
    "    \n",
    "    median_of_median_model = models[fileInd][medianIndices[fileInd]]\n",
    "    median_of_median_auc = np.median(np.median(aucs, axis=1))\n",
    "    mad_of_mad_auc = median_absolute_deviation(aucs, axis=1)\n",
    "#     return median_of_median_auc, mad_of_mad_auc\n",
    "    return median_of_median_auc\n",
    "    return np.average(aucs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9f96902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overfitting_performance(trial, data_dir=data_dir, input_shape=(96,), \n",
    "                            n_hidden_layers=2, n_hidden_nodes=16, \n",
    "                            activation=\"relu\", learning_rate=0.001, num_samples = 1, y_col = 'is_sig3_20', weight_decay = 0, dropout=0.2):\n",
    "    aucs = None\n",
    "    models = None\n",
    "    \n",
    "    # 60-20-20 split\n",
    "    \n",
    "    for X, y in dataset_generator(data_dir, num_samples, y_col):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "        # build model and ensure that parameters passed in are within the normal range\n",
    "        # if we don't type cast as integers, bayesian optimizer will guess float values\n",
    "        model = build_mlp_model(input_shape, \n",
    "                                trial.suggest_int('n_hidden_layers', 0, 8), \n",
    "                                trial.suggest_int('n_hidden_nodes', 1, 128), \n",
    "                                trial.suggest_categorical(\"activation\", [\"relu\", \"sigmoid\", \"softmax\"]), \n",
    "                                trial.suggest_float('learning_rate', 1e-6, 1e-1), \n",
    "                                trial.suggest_float('weight_decay', 0, 1e-1))\n",
    "#             model = build_mlp_model(input_shape, \n",
    "#                         trial.suggest_float('learning_rate', 1e-6, 1e-1), \n",
    "#                         trial.suggest_float('weight_decay', 0, 1e-1),\n",
    "#                         trial.suggest_float('dropout', 0, 5e-1))\n",
    "\n",
    "        model.fit(X_train, y_train, \n",
    "                  validation_data=(X_val, y_val), \n",
    "                  epochs=1000, batch_size=32, verbose=0,\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(monitor='val_auc', patience=5)])\n",
    "\n",
    "        # evaluate\n",
    "        aucs = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "        \n",
    "        models = model\n",
    "\n",
    "        \n",
    "    return aucs\n",
    "    return np.average(aucs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "512b22de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-24 14:04:27,683]\u001b[0m A new study created in memory with name: no-name-933954ca-d890-4e79-acb4-fb25b35e1993\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study_20 = optuna.create_study(direction='maximize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33089c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_3508\\1547987928.py:56: DeprecationWarning: `median_absolute_deviation` is deprecated, use `median_abs_deviation` instead!\n",
      "\n",
      "To preserve the existing default behavior, use\n",
      "`scipy.stats.median_abs_deviation(..., scale=1/1.4826)`.\n",
      "The value 1.4826 is not numerically precise for scaling\n",
      "with a normal distribution. For a numerically precise value, use\n",
      "`scipy.stats.median_abs_deviation(..., scale='normal')`.\n",
      "\n",
      "  mad_of_mad_auc = median_absolute_deviation(aucs, axis=1)\n",
      "\u001b[32m[I 2022-06-24 14:05:03,778]\u001b[0m Trial 0 finished with value: 0.557580292224884 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 28, 'activation': 'sigmoid', 'learning_rate': 0.053395658434871276, 'weight_decay': 0.0074501225498356805}. Best is trial 0 with value: 0.557580292224884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_3508\\1547987928.py:56: DeprecationWarning: `median_absolute_deviation` is deprecated, use `median_abs_deviation` instead!\n",
      "\n",
      "To preserve the existing default behavior, use\n",
      "`scipy.stats.median_abs_deviation(..., scale=1/1.4826)`.\n",
      "The value 1.4826 is not numerically precise for scaling\n",
      "with a normal distribution. For a numerically precise value, use\n",
      "`scipy.stats.median_abs_deviation(..., scale='normal')`.\n",
      "\n",
      "  mad_of_mad_auc = median_absolute_deviation(aucs, axis=1)\n",
      "\u001b[32m[I 2022-06-24 14:05:29,347]\u001b[0m Trial 1 finished with value: 0.5 and parameters: {'n_hidden_layers': 6, 'n_hidden_nodes': 31, 'activation': 'softmax', 'learning_rate': 0.054735720147404666, 'weight_decay': 0.08485234772320233}. Best is trial 0 with value: 0.557580292224884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_3508\\1547987928.py:56: DeprecationWarning: `median_absolute_deviation` is deprecated, use `median_abs_deviation` instead!\n",
      "\n",
      "To preserve the existing default behavior, use\n",
      "`scipy.stats.median_abs_deviation(..., scale=1/1.4826)`.\n",
      "The value 1.4826 is not numerically precise for scaling\n",
      "with a normal distribution. For a numerically precise value, use\n",
      "`scipy.stats.median_abs_deviation(..., scale='normal')`.\n",
      "\n",
      "  mad_of_mad_auc = median_absolute_deviation(aucs, axis=1)\n",
      "\u001b[32m[I 2022-06-24 14:05:52,504]\u001b[0m Trial 2 finished with value: 0.4467852860689163 and parameters: {'n_hidden_layers': 1, 'n_hidden_nodes': 60, 'activation': 'softmax', 'learning_rate': 0.002996511837600134, 'weight_decay': 0.03508479126718724}. Best is trial 0 with value: 0.557580292224884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_3508\\1547987928.py:56: DeprecationWarning: `median_absolute_deviation` is deprecated, use `median_abs_deviation` instead!\n",
      "\n",
      "To preserve the existing default behavior, use\n",
      "`scipy.stats.median_abs_deviation(..., scale=1/1.4826)`.\n",
      "The value 1.4826 is not numerically precise for scaling\n",
      "with a normal distribution. For a numerically precise value, use\n",
      "`scipy.stats.median_abs_deviation(..., scale='normal')`.\n",
      "\n",
      "  mad_of_mad_auc = median_absolute_deviation(aucs, axis=1)\n",
      "\u001b[32m[I 2022-06-24 14:06:18,151]\u001b[0m Trial 3 finished with value: 0.5 and parameters: {'n_hidden_layers': 7, 'n_hidden_nodes': 9, 'activation': 'relu', 'learning_rate': 0.04577476424473873, 'weight_decay': 0.05148150105102478}. Best is trial 0 with value: 0.557580292224884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_3508\\1547987928.py:56: DeprecationWarning: `median_absolute_deviation` is deprecated, use `median_abs_deviation` instead!\n",
      "\n",
      "To preserve the existing default behavior, use\n",
      "`scipy.stats.median_abs_deviation(..., scale=1/1.4826)`.\n",
      "The value 1.4826 is not numerically precise for scaling\n",
      "with a normal distribution. For a numerically precise value, use\n",
      "`scipy.stats.median_abs_deviation(..., scale='normal')`.\n",
      "\n",
      "  mad_of_mad_auc = median_absolute_deviation(aucs, axis=1)\n",
      "\u001b[32m[I 2022-06-24 14:06:47,746]\u001b[0m Trial 4 finished with value: 0.5 and parameters: {'n_hidden_layers': 4, 'n_hidden_nodes': 19, 'activation': 'relu', 'learning_rate': 0.0452209132443966, 'weight_decay': 0.06642691943672645}. Best is trial 0 with value: 0.557580292224884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstudy_20\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_mlp_performance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m study_20\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis feature will be removed in v4.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mcompute_mlp_performance\u001b[1;34m(trial, data_dir, n_folds, input_shape, n_hidden_layers, n_hidden_nodes, activation, learning_rate, num_samples, y_col, weight_decay, dropout)\u001b[0m\n\u001b[0;32m     22\u001b[0m             model \u001b[38;5;241m=\u001b[39m build_mlp_model(input_shape, \n\u001b[0;32m     23\u001b[0m                                     trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_hidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m8\u001b[39m), \n\u001b[0;32m     24\u001b[0m                                     trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_hidden_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m), \n\u001b[0;32m     25\u001b[0m                                     trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \n\u001b[0;32m     26\u001b[0m                                     trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m), \n\u001b[0;32m     27\u001b[0m                                     trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#             model = build_mlp_model(input_shape, \u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#                         trial.suggest_float('learning_rate', 1e-6, 1e-1), \u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#                         trial.suggest_float('weight_decay', 0, 1e-1),\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#                         trial.suggest_float('dropout', 0, 5e-1))\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m             \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_auc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;66;03m# evaluate\u001b[39;00m\n\u001b[0;32m     39\u001b[0m             y_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study_20.optimize(compute_mlp_performance, n_trials=100)\n",
    "study_20.best_trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
